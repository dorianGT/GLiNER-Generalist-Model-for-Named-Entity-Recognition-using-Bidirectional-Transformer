{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "téléchargez le json https://huggingface.co/datasets/Universal-NER/Pile-NER-type/blob/main/train.json\n",
    "mettez le au meme niveau que ce script\n",
    "puis éxécutez une fois le bloc suivant puis commentez le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45889/45889 [01:08<00:00, 672.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'ner_0', 'conversations': [{'from': 'human', 'value': 'Text: Q:\\n\\nPosition character based on enemy coordinates in lua\\n\\nI have written a function here which should turn my character based on enemy coordinates but it\\'s not perfect because it does not always turn where I want it to and perhaps there is a better way of writing it\\nlocal myPosition = {x = 350, y = 355}\\nlocal enemyPosition = {x = 352, y = 354}\\nlocal xValue, yValue, xDir, yDir, dir\\n\\nif myPosition.x > enemyPosition.x then\\n    xValue = myPosition.x - enemyPosition.x\\nelseif myPosition.x < enemyPosition.x then\\n    xValue = myPosition.x - enemyPosition.x\\nelse\\n    xValue = 0\\nend\\n\\nif myPosition.y > enemyPosition.y then\\n    yValue = myPosition.y - enemyPosition.y\\nelseif myPosition.y < enemyPosition.y then\\n    yValue = myPosition.y - enemyPosition.y\\nelse\\n    yValue = 0\\nend\\n\\nif xValue < 0 then\\n    xDir = \"TURN RIGHT\"\\nelseif xValue > 0 then\\n    xDir = \"TURN LEFT\"\\nend\\n\\nif yValue < 0 then\\n    yDir = \"TURN DOWN\"\\nelseif yValue > 0 then\\n    yDir = \"TURN UP\"\\nend\\n\\nif xValue > yValue then\\n    dir = xDir\\nelseif xValue \\n    dir = yDir\\nend\\n\\nprint(\"Turn: \" .. dir)\\n\\nAnd here you have some pictures to further illustrate what I have in mind:\\n\\nAs you can see on the pictures, direction depends on the higher number.'}, {'from': 'gpt', 'value': \"I've read this text.\"}, {'from': 'human', 'value': 'What describes programming concept in the text?'}, {'from': 'gpt', 'value': '[\"function\"]'}, {'from': 'human', 'value': 'What describes programming language in the text?'}, {'from': 'gpt', 'value': '[\"lua\"]'}, {'from': 'human', 'value': 'What describes database in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes variable in the text?'}, {'from': 'gpt', 'value': '[\"myPosition\", \"enemyPosition\", \"xValue\", \"yValue\", \"xDir\", \"yDir\", \"dir\"]'}, {'from': 'human', 'value': 'What describes Date in the text?'}, {'from': 'gpt', 'value': '[]'}]}\n",
      "dataset size: 45889\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Loads data from a JSON file.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenizes the input text into a list of tokens.\"\"\"\n",
    "    return re.findall(r'\\w+(?:[-_]\\w+)*|\\S', text)\n",
    "\n",
    "def extract_entity_spans(entry):\n",
    "    \"\"\"Extracts entity spans from an entry.\"\"\"\n",
    "    len_start = len(\"What describes \")\n",
    "    len_end = len(\" in the text?\")\n",
    "    entity_types, entity_texts, negative = [], [], []\n",
    "\n",
    "    for c in entry['conversations']:\n",
    "        if c['from'] == 'human' and c['value'].startswith('Text: '):\n",
    "            text = c['value'][len('Text: '):]\n",
    "            tokenized_text = tokenize_text(text)\n",
    "        elif c['from'] == 'human' and c['value'].startswith('What describes '):\n",
    "            entity_type = c['value'][len_start:-len_end]\n",
    "            entity_types.append(entity_type)\n",
    "        elif c['from'] == 'gpt' and c['value'].startswith('['):\n",
    "            if c['value'] == '[]':\n",
    "                negative.append(entity_types.pop())\n",
    "                continue\n",
    "            texts_ents = ast.literal_eval(c['value'])\n",
    "            entity_texts.extend(texts_ents)\n",
    "            num_repeat = len(texts_ents) - 1\n",
    "            entity_types.extend([entity_types[-1]] * num_repeat)\n",
    "\n",
    "    entity_spans = []\n",
    "    for j, entity_text in enumerate(entity_texts):\n",
    "        entity_tokens = tokenize_text(entity_text)\n",
    "        matches = []\n",
    "        for i in range(len(tokenized_text) - len(entity_tokens) + 1):\n",
    "            if \" \".join(tokenized_text[i:i + len(entity_tokens)]).lower() == \" \".join(entity_tokens).lower():\n",
    "                matches.append((i, i + len(entity_tokens) - 1, entity_types[j]))\n",
    "        if matches:\n",
    "            entity_spans.extend(matches)\n",
    "\n",
    "    return {\"tokenized_text\": tokenized_text, \"ner\": entity_spans, \"negative\": negative}\n",
    "\n",
    "def process_data(data):\n",
    "    \"\"\"Processes a list of data entries to extract entity spans.\"\"\"\n",
    "    all_data = [extract_entity_spans(entry) for entry in tqdm(data)]\n",
    "    return all_data\n",
    "\n",
    "def save_data_to_file(data, filepath):\n",
    "    \"\"\"Saves the processed data to a JSON file.\"\"\"\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # download the pile-ner data: \"wget https://huggingface.co/datasets/Universal-NER/Pile-NER-type/blob/main/train.json\"\n",
    "    path_pile_ner = 'train.json'\n",
    "    data = load_data(path_pile_ner)\n",
    "    processed_data = process_data(data)\n",
    "    save_data_to_file(processed_data, 'pilener_train.json')\n",
    "    print(data[0])\n",
    "    print(\"dataset size:\", len(processed_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Processing Data: 100%|██████████| 45889/45889 [02:58<00:00, 256.69entry/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def prepare_data_for_training(processed_data, tokenizer, max_length=128):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "\n",
    "    for entry in tqdm(processed_data, desc=\"Processing Data\", unit=\"entry\"):\n",
    "        tokenized_text = entry[\"tokenized_text\"]\n",
    "        ner_spans = entry[\"ner\"]\n",
    "        \n",
    "        # Tokenize the entire text\n",
    "        encoded = tokenizer(\" \".join(tokenized_text), padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate label tensor\n",
    "        label_tensor = torch.zeros(max_length, dtype=torch.long)\n",
    "        for start, end, entity_type in ner_spans:\n",
    "            if start < max_length and end < max_length:\n",
    "                label_tensor[start:end + 1] = 1  # Mark entity spans as positive (or specific to entity types if needed)\n",
    "        \n",
    "        input_ids.append(encoded[\"input_ids\"][0])\n",
    "        attention_masks.append(encoded[\"attention_mask\"][0])\n",
    "        labels.append(label_tensor)\n",
    "\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks), torch.stack(labels)\n",
    "\n",
    "# Charger les données générées précédemment\n",
    "with open('pilener_train.json', 'r') as f:\n",
    "    processed_data = json.load(f)\n",
    "\n",
    "# Charger le tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Préparer les données avec suivi d'avancement\n",
    "input_ids, attention_masks, labels = prepare_data_for_training(processed_data, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure des données\n",
    "Les données retournées par prepare_data_for_training sont des tensors PyTorch. Voici les détails :\n",
    "\n",
    "input_ids : Contient les identifiants tokenisés correspondant au texte d'entrée.\n",
    "\n",
    "Taille : (nombre_entrées, max_length)\n",
    "Type : torch.Tensor (entiers longs)\n",
    "Exemple : Les premiers identifiants d'entrée (transformés par le tokenizer).\n",
    "attention_masks : Contient des masques indiquant les positions valides des tokens dans chaque séquence (1 pour les tokens valides, 0 pour le padding).\n",
    "\n",
    "Taille : (nombre_entrées, max_length)\n",
    "Type : torch.Tensor (entiers longs)\n",
    "Exemple : Masque associé à input_ids.\n",
    "labels : Contient les étiquettes pour les entités NER. Chaque position correspond à une classe (0 pour le non-entité, 1 ou plus pour des entités spécifiques selon vos définitions).\n",
    "\n",
    "Taille : (nombre_entrées, max_length)\n",
    "Type : torch.Tensor (entiers longs)\n",
    "Exemple : Vecteur avec 0 (non-entité) ou 1+ (entités)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de input_ids : torch.Size([45889, 128])\n",
      "Forme de attention_masks : torch.Size([45889, 128])\n",
      "Forme de labels : torch.Size([45889, 128])\n",
      "\n",
      "Exemple de input_ids (première entrée) :\n",
      "tensor([  101,  1053,  1024,  2597,  2839,  2241,  2006,  4099, 12093,  1999,\n",
      "        11320,  2050,  1045,  2031,  2517,  1037,  3853,  2182,  2029,  2323,\n",
      "         2735,  2026,  2839,  2241,  2006,  4099, 12093,  2021,  2009,  1005,\n",
      "         1055,  2025,  3819,  2138,  2009,  2515,  2025,  2467,  2735,  2073,\n",
      "         1045,  2215,  2009,  2000,  1998,  3383,  2045,  2003,  1037,  2488,\n",
      "         2126,  1997,  3015,  2009,  2334,  2026, 26994,  1027,  1063,  1060,\n",
      "         1027,  8698,  1010,  1061,  1027, 26271,  1065,  2334,  4099, 26994,\n",
      "         1027,  1063,  1060,  1027, 28906,  1010,  1061,  1027, 27878,  1065,\n",
      "         2334, 15566,  2389,  5657,  1010,  1061, 10175,  5657,  1010,  1060,\n",
      "         4305,  2099,  1010, 21076,  4313,  1010, 16101,  2065,  2026, 26994,\n",
      "         1012,  1060,  1028,  4099, 26994,  1012,  1060,  2059, 15566,  2389,\n",
      "         5657,  1027,  2026, 26994,  1012,  1060,  1011,  4099, 26994,  1012,\n",
      "         1060,  2842, 10128,  2026, 26994,  1012,  1060,   102])\n",
      "\n",
      "Exemple de attention_masks (première entrée) :\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "Exemple de labels (première entrée) :\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Afficher les formes des tensors\n",
    "print(f\"Forme de input_ids : {input_ids.shape}\")\n",
    "print(f\"Forme de attention_masks : {attention_masks.shape}\")\n",
    "print(f\"Forme de labels : {labels.shape}\")\n",
    "\n",
    "# Afficher un exemple\n",
    "print(\"\\nExemple de input_ids (première entrée) :\")\n",
    "print(input_ids[0])\n",
    "\n",
    "print(\"\\nExemple de attention_masks (première entrée) :\")\n",
    "print(attention_masks[0])\n",
    "\n",
    "print(\"\\nExemple de labels (première entrée) :\")\n",
    "print(labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [03:36<00:00,  1.06it/s, loss=4.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Average Loss: 4.759223673541473\n",
      "Epoch 2/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [03:36<00:00,  1.06it/s, loss=4.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Average Loss: 4.752854284761254\n",
      "Epoch 3/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [03:36<00:00,  1.06it/s, loss=4.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Average Loss: 4.752221913316885\n",
      "Epoch 4/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [03:36<00:00,  1.06it/s, loss=4.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Average Loss: 4.7512025000226545\n",
      "Epoch 5/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [03:37<00:00,  1.05it/s, loss=4.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Average Loss: 4.753131791493778\n",
      "Epoch 6/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [03:38<00:00,  1.05it/s, loss=4.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Average Loss: 4.7504627152821906\n",
      "Epoch 7/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [03:36<00:00,  1.06it/s, loss=4.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Average Loss: 4.75010921861407\n",
      "Epoch 8/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [03:32<00:00,  1.08it/s, loss=4.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Average Loss: 4.749693093862075\n",
      "Epoch 9/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [03:31<00:00,  1.08it/s, loss=4.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Average Loss: 4.749861021749838\n",
      "Epoch 10/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [03:32<00:00,  1.08it/s, loss=4.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Average Loss: 4.749651105122795\n",
      "Entraînement terminé !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm  # Import de tqdm\n",
    "\n",
    "class GLiNERModel(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased'):\n",
    "        super(GLiNERModel, self).__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        \n",
    "        # Ajuster la couche pour gérer les dimensions concaténées (2 * hidden_size)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(2 * self.bert.config.hidden_size, 768),  # 2x car on concatène deux embeddings\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 1)  # Pour générer un score pour chaque span\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        span_scores = self.compute_span_scores(hidden_states)  # Tenseur (batch_size, seq_len, seq_len)\n",
    "        return span_scores\n",
    "\n",
    "    def compute_span_scores(self, hidden_states):\n",
    "        batch_size, seq_len, hidden_size = hidden_states.size()\n",
    "        max_span_length = 12  # Limite de la longueur du span\n",
    "        scores = torch.full((batch_size, seq_len, seq_len), float(0), device=hidden_states.device)\n",
    "\n",
    "        for b in range(batch_size):  # Parcourir chaque exemple dans le batch\n",
    "            for i in range(seq_len):  # Pour chaque token dans la séquence\n",
    "                for j in range(i + 1, min(i + max_span_length + 1, seq_len)):  # Limiter la longueur du span\n",
    "                    span_embedding = torch.cat((hidden_states[b, i], hidden_states[b, j]), dim=-1)\n",
    "                    span_score = self.ffn(span_embedding)  # Calculer le score du span\n",
    "                    scores[b, i, j] = span_score.squeeze(-1)  # Stocker le score dans le tenseur\n",
    "        return scores\n",
    "\n",
    "\n",
    "# Créer un DataLoader (assurez-vous que input_ids, attention_masks et labels sont définis)\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Charger uniquement une portion des données\n",
    "portion = 0.005  # Utilisez 10% des données\n",
    "subset_size = int(len(dataset) * portion)\n",
    "mini_dataset = TensorDataset(\n",
    "    input_ids[:subset_size], attention_masks[:subset_size], labels[:subset_size]\n",
    ")\n",
    "\n",
    "# Créer un DataLoader pour le mini-dataset\n",
    "mini_dataloader = DataLoader(mini_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialiser le modèle et les paramètres d'entraînement\n",
    "model = GLiNERModel(pretrained_model='bert-base-uncased')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Boucle d'entraînement ajustée\n",
    "epochs = 10\n",
    "# Boucle d'entraînement ajustée avec mini_dataloader\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "    with tqdm(total=len(mini_dataloader), desc=\"Training\") as pbar:\n",
    "        for batch in mini_dataloader:\n",
    "            batch_input_ids, batch_attention_masks, batch_labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_input_ids, batch_attention_masks)\n",
    "            \n",
    "            # Réorganiser les dimensions pour correspondre aux labels\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            batch_labels = batch_labels.view(-1)\n",
    "            \n",
    "            # Calculer la perte\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass et optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Mettre à jour la barre de progression\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Average Loss: {total_loss / len(mini_dataloader)}\")\n",
    "\n",
    "print(\"Entraînement terminé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test du modèle ===\n",
      "Dimensions des scores : torch.Size([1, 128, 128])\n",
      "\n",
      "Exemple 1 :\n",
      "Scores des spans (troncature) :\n",
      "[ 0.        -4.4619265 -4.7122035 -4.380763  -4.728676 ]\n",
      "[  0.         0.       -14.446016 -14.31452  -14.511815]\n",
      "[  0.          0.          0.        -14.1396475 -14.337201 ]\n",
      "[  0.         0.         0.         0.       -14.261844]\n",
      "[0. 0. 0. 0. 0.]\n",
      "\n",
      "Indices prédits pour les spans :\n",
      "Exemple 1 : Start=0, End=0, Score=0.0\n",
      "Texte prédit pour le meilleur span : ##f ) or azathioprine, often associated with\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, dataloader, device):\n",
    "    model.eval()  # Passer en mode évaluation\n",
    "    print(\"\\n=== Test du modèle ===\")\n",
    "    \n",
    "    with torch.no_grad():  # Désactiver la backpropagation\n",
    "        for batch in dataloader:\n",
    "            batch_input_ids, batch_attention_masks, batch_labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Obtenir les scores des spans\n",
    "            outputs = model(batch_input_ids, batch_attention_masks)\n",
    "            \n",
    "            # Afficher les dimensions des scores\n",
    "            print(\"Dimensions des scores :\", outputs.size())  # (batch_size, seq_len, seq_len)\n",
    "            \n",
    "            # Afficher quelques exemples de scores\n",
    "            batch_size, seq_len, _ = outputs.size()\n",
    "            for b in range(batch_size):\n",
    "                print(f\"\\nExemple {b+1} :\")\n",
    "                print(\"Scores des spans (troncature) :\")\n",
    "                for i in range(min(seq_len, 5)):  # Limiter l'affichage à 5 tokens pour la lisibilité\n",
    "                    print(outputs[b, i, :min(seq_len, 5)].cpu().numpy())\n",
    "            \n",
    "            # Tester une prédiction simple : trouver le span avec le score maximal\n",
    "            max_scores, indices = torch.max(outputs.view(batch_size, -1), dim=1)\n",
    "            start_indices = indices // seq_len\n",
    "            end_indices = indices % seq_len\n",
    "            print(\"\\nIndices prédits pour les spans :\")\n",
    "            for b in range(batch_size):\n",
    "                print(f\"Exemple {b+1} : Start={start_indices[b].item()}, End={end_indices[b].item()}, Score={max_scores[b].item()}\")\n",
    "            \n",
    "            break  # Tester uniquement sur le premier batch pour l'instant\n",
    "\n",
    "# Appel du test après l'entraînement\n",
    "test_model(model, mini_dataloader, device)\n",
    "predicted_text = model.tokenizer.decode(batch_input_ids[0, 80:91].tolist())\n",
    "print(f\"Texte prédit pour le meilleur span : {predicted_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.1311],\n",
      "        [-0.0504],\n",
      "        [-0.0652],\n",
      "        [-0.2085],\n",
      "        [-0.1837],\n",
      "        [-0.1692],\n",
      "        [-0.1602],\n",
      "        [-0.1907],\n",
      "        [-0.0938],\n",
      "        [-0.1070],\n",
      "        [-0.2364],\n",
      "        [-0.2298],\n",
      "        [-0.1192],\n",
      "        [-0.1750],\n",
      "        [-0.2772],\n",
      "        [-0.0874],\n",
      "        [-0.2078],\n",
      "        [-0.2143],\n",
      "        [-0.1640],\n",
      "        [-0.2102],\n",
      "        [-0.2847],\n",
      "        [-0.1154],\n",
      "        [-0.0959],\n",
      "        [-0.0901],\n",
      "        [-0.0634],\n",
      "        [-0.1718],\n",
      "        [-0.0674],\n",
      "        [-0.0672],\n",
      "        [-0.0182],\n",
      "        [-0.1385],\n",
      "        [-0.1543],\n",
      "        [-0.0863],\n",
      "        [-0.1317],\n",
      "        [-0.1460],\n",
      "        [-0.1849],\n",
      "        [-0.1721]], grad_fn=<StackBackward0>)]\n",
      "Example 0: torch.Size([36, 1])\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class GLiNERModel(nn.Module):\n",
    "#     def __init__(self, pretrained_model='bert-base-uncased'):\n",
    "#         super(GLiNERModel, self).__init__()\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "#         self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        \n",
    "#         # Ajuster la couche pour gérer les dimensions concaténées (2 * hidden_size)\n",
    "#         self.ffn = nn.Sequential(\n",
    "#             nn.Linear(2 * self.bert.config.hidden_size, 768),  # 2x car on concatène deux embeddings\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(768, 1)  # Pour générer un score pour chaque span\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # Encodage du texte avec BERT\n",
    "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_states = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "#         # Représentation des spans\n",
    "#         span_scores = self.compute_span_scores(hidden_states)\n",
    "        \n",
    "#         return span_scores\n",
    "\n",
    "#     def compute_span_scores(self, hidden_states):\n",
    "#         \"\"\"\n",
    "#         Cette fonction calcule la représentation des spans et leur score.\n",
    "#         Pour simplifier, nous utilisons les premiers et derniers tokens du span pour calculer le score.\n",
    "#         \"\"\"\n",
    "#         batch_size, seq_len, hidden_size = hidden_states.size()\n",
    "#         span_scores = []\n",
    "        \n",
    "#         for b in range(batch_size):  # Parcourir chaque exemple dans le batch\n",
    "#             example_scores = []\n",
    "#             for i in range(seq_len - 1):  # Pour chaque token dans la séquence\n",
    "#                 for j in range(i + 1, min(i + 13, seq_len)):  # Limiter la longueur du span\n",
    "#                     span_embedding = torch.cat((hidden_states[b, i], hidden_states[b, j]), dim=-1)\n",
    "#                     span_score = self.ffn(span_embedding)  # Calculer le score du span\n",
    "#                     example_scores.append(span_score)\n",
    "#             span_scores.append(torch.stack(example_scores))\n",
    "        \n",
    "#         return span_scores\n",
    "\n",
    "# # Exemple d'utilisation du modèle avec un input fictif\n",
    "# model = GLiNERModel()\n",
    "\n",
    "# # Exemple d'input\n",
    "# text = [\"Alain Farley works at McGill University\"]\n",
    "# inputs = model.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# # Forward pass\n",
    "# span_scores = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "# print(span_scores)\n",
    "# # Afficher les scores des spans\n",
    "# for i, scores in enumerate(span_scores):\n",
    "#     print(f\"Example {i}: {scores.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vincentorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

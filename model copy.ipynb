{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exemple traité ===\n",
      "Tokenized Text:\n",
      "['Q', ':', 'Position', 'character', 'based', 'on', 'enemy', 'coordinates', 'in', 'lua', 'I', 'have', 'written', 'a', 'function', 'here', 'which', 'should', 'turn', 'my', 'character', 'based', 'on', 'enemy', 'coordinates', 'but', 'it', \"'\", 's', 'not', 'perfect', 'because', 'it', 'does', 'not', 'always', 'turn', 'where', 'I', 'want', 'it', 'to', 'and', 'perhaps', 'there', 'is', 'a', 'better', 'way', 'of', 'writing', 'it', 'local', 'myPosition', '=', '{', 'x', '=', '350', ',', 'y', '=', '355', '}', 'local', 'enemyPosition', '=', '{', 'x', '=', '352', ',', 'y', '=', '354', '}', 'local', 'xValue', ',', 'yValue', ',', 'xDir', ',', 'yDir', ',', 'dir', 'if', 'myPosition', '.', 'x', '>', 'enemyPosition', '.', 'x', 'then', 'xValue', '=', 'myPosition', '.', 'x', '-', 'enemyPosition', '.', 'x', 'elseif', 'myPosition', '.', 'x', '<', 'enemyPosition', '.', 'x', 'then', 'xValue', '=', 'myPosition', '.', 'x', '-', 'enemyPosition', '.', 'x', 'else', 'xValue', '=', '0', 'end', 'if', 'myPosition', '.', 'y', '>', 'enemyPosition', '.', 'y', 'then', 'yValue', '=', 'myPosition', '.', 'y', '-', 'enemyPosition', '.', 'y', 'elseif', 'myPosition', '.', 'y', '<', 'enemyPosition', '.', 'y', 'then', 'yValue', '=', 'myPosition', '.', 'y', '-', 'enemyPosition', '.', 'y', 'else', 'yValue', '=', '0', 'end', 'if', 'xValue', '<', '0', 'then', 'xDir', '=', '\"', 'TURN', 'RIGHT', '\"', 'elseif', 'xValue', '>', '0', 'then', 'xDir', '=', '\"', 'TURN', 'LEFT', '\"', 'end', 'if', 'yValue', '<', '0', 'then', 'yDir', '=', '\"', 'TURN', 'DOWN', '\"', 'elseif', 'yValue', '>', '0', 'then', 'yDir', '=', '\"', 'TURN', 'UP', '\"', 'end', 'if', 'xValue', '>', 'yValue', 'then', 'dir', '=', 'xDir', 'elseif', 'xValue', 'dir', '=', 'yDir', 'end', 'print', '(', '\"', 'Turn', ':', '\"', '.', '.', 'dir', ')', 'And', 'here', 'you', 'have', 'some', 'pictures', 'to', 'further', 'illustrate', 'what', 'I', 'have', 'in', 'mind', ':', 'As', 'you', 'can', 'see', 'on', 'the', 'pictures', ',', 'direction', 'depends', 'on', 'the', 'higher', 'number', '.']\n",
      "\n",
      "NER Spans:\n",
      " - Start: 14, End: 14, Entity Type: programming concept\n",
      " - Start: 9, End: 9, Entity Type: programming language\n",
      " - Start: 53, End: 53, Entity Type: variable\n",
      " - Start: 87, End: 87, Entity Type: variable\n",
      " - Start: 97, End: 97, Entity Type: variable\n",
      " - Start: 105, End: 105, Entity Type: variable\n",
      " - Start: 115, End: 115, Entity Type: variable\n",
      " - Start: 128, End: 128, Entity Type: variable\n",
      " - Start: 138, End: 138, Entity Type: variable\n",
      " - Start: 146, End: 146, Entity Type: variable\n",
      " - Start: 156, End: 156, Entity Type: variable\n",
      " - Start: 65, End: 65, Entity Type: variable\n",
      " - Start: 91, End: 91, Entity Type: variable\n",
      " - Start: 101, End: 101, Entity Type: variable\n",
      " - Start: 109, End: 109, Entity Type: variable\n",
      " - Start: 119, End: 119, Entity Type: variable\n",
      " - Start: 132, End: 132, Entity Type: variable\n",
      " - Start: 142, End: 142, Entity Type: variable\n",
      " - Start: 150, End: 150, Entity Type: variable\n",
      " - Start: 160, End: 160, Entity Type: variable\n",
      " - Start: 77, End: 77, Entity Type: variable\n",
      " - Start: 95, End: 95, Entity Type: variable\n",
      " - Start: 113, End: 113, Entity Type: variable\n",
      " - Start: 123, End: 123, Entity Type: variable\n",
      " - Start: 169, End: 169, Entity Type: variable\n",
      " - Start: 180, End: 180, Entity Type: variable\n",
      " - Start: 215, End: 215, Entity Type: variable\n",
      " - Start: 223, End: 223, Entity Type: variable\n",
      " - Start: 79, End: 79, Entity Type: variable\n",
      " - Start: 136, End: 136, Entity Type: variable\n",
      " - Start: 154, End: 154, Entity Type: variable\n",
      " - Start: 164, End: 164, Entity Type: variable\n",
      " - Start: 192, End: 192, Entity Type: variable\n",
      " - Start: 203, End: 203, Entity Type: variable\n",
      " - Start: 217, End: 217, Entity Type: variable\n",
      " - Start: 81, End: 81, Entity Type: variable\n",
      " - Start: 173, End: 173, Entity Type: variable\n",
      " - Start: 184, End: 184, Entity Type: variable\n",
      " - Start: 221, End: 221, Entity Type: variable\n",
      " - Start: 83, End: 83, Entity Type: variable\n",
      " - Start: 196, End: 196, Entity Type: variable\n",
      " - Start: 207, End: 207, Entity Type: variable\n",
      " - Start: 226, End: 226, Entity Type: variable\n",
      " - Start: 85, End: 85, Entity Type: variable\n",
      " - Start: 219, End: 219, Entity Type: variable\n",
      " - Start: 224, End: 224, Entity Type: variable\n",
      " - Start: 236, End: 236, Entity Type: variable\n",
      "\n",
      "Negative Entities:\n",
      "['database', 'Date']\n"
     ]
    }
   ],
   "source": [
    "# Charger les données traitées pour inspection\n",
    "with open('pilener_train.json', 'r') as f:\n",
    "    processed_data = json.load(f)\n",
    "\n",
    "# Afficher un exemple\n",
    "example_idx = 0  # Modifier cet indice pour voir d'autres exemples\n",
    "example = processed_data[example_idx]\n",
    "\n",
    "# Afficher avec une mise en forme claire\n",
    "print(\"=== Exemple traité ===\")\n",
    "print(\"Tokenized Text:\")\n",
    "print(example['tokenized_text'])\n",
    "print(\"\\nNER Spans:\")\n",
    "for span in example['ner']:\n",
    "    print(f\" - Start: {span[0]}, End: {span[1]}, Entity Type: {span[2]}\")\n",
    "print(\"\\nNegative Entities:\")\n",
    "print(example['negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faut allez à lendroit ou est defini le model puis vous pouvez exec la suite dans lordre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GLiNER(\n",
       "  (encoder): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (entity_ffn): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (span_ffn): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (loss_fn): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GLiNER(pretrained_model_name=\"microsoft/deberta-v3-base\", span_max_length=2, hidden_size=768)\n",
    "# Initialiser le modèle\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  10%|▉         | 4387/45889 [00:14<02:15, 307.01entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  34%|███▍      | 15527/45889 [00:50<01:38, 307.11entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  38%|███▊      | 17432/45889 [00:56<01:30, 313.06entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  53%|█████▎    | 24201/45889 [01:18<01:08, 318.48entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  64%|██████▍   | 29356/45889 [01:35<00:52, 316.41entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  73%|███████▎  | 33392/45889 [01:47<00:40, 307.69entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  76%|███████▌  | 34913/45889 [01:52<00:35, 312.46entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  88%|████████▊ | 40325/45889 [02:10<00:16, 343.83entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|██████████| 45889/45889 [02:26<00:00, 312.95entry/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def prepare_data_for_training(processed_data, model, max_length=64, max_entity_per_seq = 10):\n",
    "    input_ids, labels, entity_tensors, attention_masks = [], [], [], []\n",
    "    entity_masks, sentence_masks = [], []\n",
    "\n",
    "    # Créer un mapping des types d'entités vers des entiers\n",
    "    entity_types = {entity for entry in processed_data for _, _, entity in entry[\"ner\"]}\n",
    "    special_tokens = [f\"[ENT] {entity}\" for entity in entity_types]\n",
    "    model.tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "    model.encoder.resize_token_embeddings(len(model.tokenizer))\n",
    "\n",
    "    for entry in tqdm(processed_data, desc=\"Processing Data\", unit=\"entry\"):\n",
    "        tokenized_text = entry[\"tokenized_text\"]\n",
    "        ner_spans = entry[\"ner\"]\n",
    "                \n",
    "        # Générer le tensor de labels\n",
    "        label_tensor = torch.zeros(max_length, dtype=torch.long)\n",
    "        current_entity_id = []\n",
    "        current_entity_str = []\n",
    "\n",
    "        for start, end, entity_type in ner_spans:\n",
    "            if start < max_length and end < max_length and len(current_entity_str)<max_entity_per_seq:\n",
    "                entity_token_id = model.tokenizer.convert_tokens_to_ids(f'[ENT] {entity_type}')\n",
    "                label_tensor[start:end + 1] = entity_token_id\n",
    "\n",
    "                if entity_token_id not in current_entity_id:\n",
    "                    current_entity_id.append(entity_token_id)\n",
    "                if entity_type not in current_entity_str:\n",
    "                    current_entity_str.append(entity_type)\n",
    "\n",
    "        entity_tokens = \" \".join(f\"[ENT] {et}\" for et in current_entity_str)\n",
    "        \n",
    "        # Tokeniser la séquence principale\n",
    "        encoded = model.tokenizer(\n",
    "            tokenized_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, \n",
    "            is_split_into_words=True, add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        word_ids = encoded.word_ids()\n",
    "        first_subtoken_ids = [\n",
    "            encoded[\"input_ids\"][0, i].item() for i, word_id in enumerate(word_ids) \n",
    "            if word_id is not None and (i == 0 or word_ids[i - 1] != word_id)\n",
    "        ]\n",
    "\n",
    "        encoded_entity = model.tokenizer(\n",
    "            entity_tokens, return_tensors=\"pt\", padding=\"max_length\", truncation=True, \n",
    "            is_split_into_words=False, add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        if len(current_entity_str) != len(encoded_entity[\"input_ids\"][0]) or len(tokenized_text) != len(first_subtoken_ids):\n",
    "            print(\"error, not same size\")\n",
    "            continue\n",
    "\n",
    "        encoded_entity = encoded_entity[\"input_ids\"][0].tolist() + [0]*(max_entity_per_seq-len(current_entity_str))\n",
    "\n",
    "        sep_id = model.tokenizer.convert_tokens_to_ids(f'[SEP]')\n",
    "\n",
    "        combined_ids = (\n",
    "            encoded_entity +\n",
    "            [sep_id] +\n",
    "            first_subtoken_ids\n",
    "        )\n",
    "\n",
    "        if len(combined_ids) != max_entity_per_seq + len(first_subtoken_ids) + 1:\n",
    "            print(\"error, not same size\")\n",
    "            continue\n",
    "\n",
    "        deleted_ids = max(len(combined_ids) - max_length,0)\n",
    "        combined_ids = combined_ids[:max_length]\n",
    "        combined_ids += [0] * (max_length - len(combined_ids))\n",
    "\n",
    "        # Créer l'attention mask\n",
    "        attention_mask = [1 if id != 0 else 0 for id in combined_ids]\n",
    "\n",
    "        # Masques spécifiques pour les entités et la phrase\n",
    "        entity_mask = [1 if i < len(current_entity_str) else 0 for i in range(len(combined_ids))]\n",
    "        sentence_mask = [1 if i > len(encoded_entity) and combined_ids[i] != 0 and combined_ids[i] != sep_id else 0 \n",
    "                         for i in range(len(combined_ids))]\n",
    "\n",
    "        # Vérification des tailles\n",
    "        if sum(entity_mask) != len(current_entity_str):\n",
    "            print(f\"Entity mask size mismatch: {sum(entity_mask)} != {len(current_entity_str)}\")\n",
    "            continue\n",
    "        if sum(sentence_mask) != len(tokenized_text)-deleted_ids:\n",
    "            print(f\"Sentence mask size mismatch: {sum(sentence_mask)} != {len(tokenized_text)-deleted_ids}\")\n",
    "            continue\n",
    "\n",
    "        current_entity_id = current_entity_id + [0]*(max_entity_per_seq-len(current_entity_str))\n",
    "\n",
    "        # Convertir les entités en un tensor\n",
    "        entity_tensor = torch.tensor(current_entity_id, dtype=torch.long)\n",
    "        \n",
    "        # Ajouter les données\n",
    "        input_ids.append(torch.tensor(combined_ids, dtype=torch.long))\n",
    "        labels.append(label_tensor)\n",
    "        entity_tensors.append(entity_tensor)\n",
    "        attention_masks.append(torch.tensor(attention_mask, dtype=torch.long))\n",
    "        entity_masks.append(torch.tensor(entity_mask, dtype=torch.long))\n",
    "        sentence_masks.append(torch.tensor(sentence_mask, dtype=torch.long))\n",
    "\n",
    "    return (\n",
    "        torch.stack(input_ids), \n",
    "        torch.stack(labels), \n",
    "        entity_tensors, \n",
    "        torch.stack(attention_masks), \n",
    "        torch.stack(entity_masks), \n",
    "        torch.stack(sentence_masks)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Charger les données générées précédemment\n",
    "with open('pilener_train.json', 'r') as f:\n",
    "    processed_data = json.load(f)\n",
    "\n",
    "# Préparer les données avec suivi d'avancement\n",
    "input_ids, labels, entity_tensors, attention_masks, entity_masks, sentence_masks = prepare_data_for_training(processed_data, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143177\n",
      "45881\n",
      "45881\n"
     ]
    }
   ],
   "source": [
    "print(len(model.tokenizer))\n",
    "print(len(input_ids))\n",
    "print(len(entity_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de input_ids : torch.Size([45881, 64])\n",
      "Forme de attention_masks : torch.Size([45881, 64])\n",
      "Forme de labels : torch.Size([45881, 64])\n",
      "Forme de entity_masks : torch.Size([45881, 64])\n",
      "Forme de sentence_masks : torch.Size([45881, 64])\n",
      "\n",
      "Exemple de input_ids (première entrée) :\n",
      "tensor([135064, 137417, 132538,      0,      0,      0,      0,      0,      0,\n",
      "             0,      2,   1729,    877,  18172,   1470,    636,    277,   4648,\n",
      "         14321,    267,  96792,    273,    286,   1223,    266,   1571,    422,\n",
      "           319,    403,    930,    312,   1470,    636,    277,   4648,  14321,\n",
      "           304,    278,    382,   1550,    298,    801,    401,    278,    490,\n",
      "           298,    489,    930,    399,    273,    409,    278,    264,    263,\n",
      "          1733,    343,    269,    266,    493,    384,    265,    898,    278,\n",
      "           588])\n",
      "\n",
      "Exemple de attention_masks (première entrée) :\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "Exemple de labels (première entrée) :\n",
      "tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "        137417,      0,      0,      0,      0, 135064,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0, 132538,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0])\n",
      "\n",
      "Exemple de entity_tensors (première entrée) :\n",
      "tensor([135064, 137417, 132538,      0,      0,      0,      0,      0,      0,\n",
      "             0])\n",
      "\n",
      "Exemple de entity_masks (première entrée) :\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "Exemple de sentence_masks (première entrée) :\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Afficher les formes des tensors principaux\n",
    "print(f\"Forme de input_ids : {input_ids.shape}\")\n",
    "print(f\"Forme de attention_masks : {attention_masks.shape}\")\n",
    "print(f\"Forme de labels : {labels.shape}\")\n",
    "print(f\"Forme de entity_masks : {entity_masks.shape}\")\n",
    "print(f\"Forme de sentence_masks : {sentence_masks.shape}\")\n",
    "\n",
    "# Afficher un exemple pour les tensors principaux\n",
    "print(\"\\nExemple de input_ids (première entrée) :\")\n",
    "print(input_ids[0])\n",
    "\n",
    "print(\"\\nExemple de attention_masks (première entrée) :\")\n",
    "print(attention_masks[0])\n",
    "\n",
    "print(\"\\nExemple de labels (première entrée) :\")\n",
    "print(labels[0])\n",
    "\n",
    "# Afficher un exemple pour les entity_tensors\n",
    "print(\"\\nExemple de entity_tensors (première entrée) :\")\n",
    "print(entity_tensors[0])\n",
    "\n",
    "# Afficher un exemple pour les entity_masks\n",
    "print(\"\\nExemple de entity_masks (première entrée) :\")\n",
    "print(entity_masks[0])\n",
    "\n",
    "# Afficher un exemple pour les sentence_masks\n",
    "print(\"\\nExemple de sentence_masks (première entrée) :\")\n",
    "print(sentence_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels, entities,entity_masks,sentence_masks , max_span_length=2):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.entity_masks = entity_masks\n",
    "        self.sentence_masks =sentence_masks\n",
    "        self.labels = labels  # Liste des labels pour chaque token\n",
    "        self.entities = entities  # Liste des entités uniques\n",
    "        self.max_span_length = max_span_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_id = self.input_ids[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        token_labels = self.labels[idx]  # Labels token-par-token\n",
    "        entity_ids = self.entities[idx]  # Entités pour cet exemple\n",
    "\n",
    "        num_tokens = len(input_id) - len(entity_ids) - 1\n",
    "        spans = [\n",
    "            (start, end)\n",
    "            for start in range(num_tokens)\n",
    "            for end in range(start, min(start + self.max_span_length, num_tokens))\n",
    "        ]\n",
    "        num_spans = len(spans)\n",
    "        num_entities = len(entity_ids)\n",
    "\n",
    "        # Matrice binaire : spans x entities\n",
    "        binary_labels = torch.zeros(num_spans, num_entities, dtype=torch.float)\n",
    "\n",
    "        for span_idx, (start, end) in enumerate(spans):\n",
    "            span_labels = token_labels[start:end + 1]\n",
    "            for entity_idx, entity_id in enumerate(entity_ids):\n",
    "                if all((label == entity_id and label != 0)  for label in span_labels):\n",
    "                    binary_labels[span_idx, entity_idx] = 1\n",
    "\n",
    "        return input_id, attention_mask, spans, entity_ids, binary_labels, sentence_masks, entity_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = int(len(input_ids) * proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID: tensor([135064, 137417, 132538,      0,      0,      0,      0,      0,      0,\n",
      "             0,      2,   1729,    877,  18172,   1470,    636,    277,   4648,\n",
      "         14321,    267,  96792,    273,    286,   1223,    266,   1571,    422,\n",
      "           319,    403,    930,    312,   1470,    636,    277,   4648,  14321,\n",
      "           304,    278,    382,   1550,    298,    801,    401,    278,    490,\n",
      "           298,    489,    930,    399,    273,    409,    278,    264,    263,\n",
      "          1733,    343,    269,    266,    493,    384,    265,    898,    278,\n",
      "           588])\n",
      "Spans: [(0, 0), (0, 1), (1, 1), (1, 2), (2, 2), (2, 3), (3, 3), (3, 4), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (6, 7), (7, 7), (7, 8), (8, 8), (8, 9), (9, 9), (9, 10), (10, 10), (10, 11), (11, 11), (11, 12), (12, 12), (12, 13), (13, 13), (13, 14), (14, 14), (14, 15), (15, 15), (15, 16), (16, 16), (16, 17), (17, 17), (17, 18), (18, 18), (18, 19), (19, 19), (19, 20), (20, 20), (20, 21), (21, 21), (21, 22), (22, 22), (22, 23), (23, 23), (23, 24), (24, 24), (24, 25), (25, 25), (25, 26), (26, 26), (26, 27), (27, 27), (27, 28), (28, 28), (28, 29), (29, 29), (29, 30), (30, 30), (30, 31), (31, 31), (31, 32), (32, 32), (32, 33), (33, 33), (33, 34), (34, 34), (34, 35), (35, 35), (35, 36), (36, 36), (36, 37), (37, 37), (37, 38), (38, 38), (38, 39), (39, 39), (39, 40), (40, 40), (40, 41), (41, 41), (41, 42), (42, 42), (42, 43), (43, 43), (43, 44), (44, 44), (44, 45), (45, 45), (45, 46), (46, 46), (46, 47), (47, 47), (47, 48), (48, 48), (48, 49), (49, 49), (49, 50), (50, 50), (50, 51), (51, 51), (51, 52), (52, 52)]\n",
      "Entity IDs: tensor([135064, 137417, 132538,      0,      0,      0,      0,      0,      0,\n",
      "             0])\n",
      "Binary Labels: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Créer le dataset\n",
    "dataset = NERDataset(input_ids[:ind], attention_masks[:ind], labels[:ind], entity_tensors[:ind],entity_masks[:ind],sentence_masks[:ind])\n",
    "\n",
    "# Exemple de récupération d'une entrée\n",
    "input_id, attention_mask, spans, entity_ids, binary_labels, sentence_masks, entity_masks = dataset[0]\n",
    "\n",
    "print(\"Input ID:\", input_id)\n",
    "print(\"Spans:\", spans)\n",
    "print(\"Entity IDs:\", entity_ids)\n",
    "print(\"Binary Labels:\", binary_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks = zip(*batch)\n",
    "\n",
    "    # Trouver les tailles maximales pour le padding\n",
    "    max_len = max(len(ids) for ids in input_ids)  # Longueur max des tokens\n",
    "    max_spans = max(len(s) for s in spans)  # Nombre max de spans\n",
    "    max_entities = max(len(e) for e in entity_ids)  # Nombre max d'entités\n",
    "\n",
    "    # Padding des input_ids et attention_masks\n",
    "    # padded_input_ids = torch.stack([\n",
    "    #     torch.cat([ids, torch.zeros(max_len - len(ids), dtype=torch.long)])\n",
    "    #     for ids in input_ids\n",
    "    # ])\n",
    "    # padded_attention_masks = torch.stack([\n",
    "    #     torch.cat([mask, torch.zeros(max_len - len(mask), dtype=torch.long)])\n",
    "    #     for mask in attention_masks\n",
    "    # ])\n",
    "\n",
    "    # Padding des spans\n",
    "    # spans = torch.stack([\n",
    "    #     torch.cat([torch.tensor(s, dtype=torch.long), torch.zeros((max_spans - len(s), 2), dtype=torch.long)])\n",
    "    #     for s in spans\n",
    "    # ])\n",
    "\n",
    "    # Padding des entity_ids\n",
    "    # padded_entity_ids = torch.stack([\n",
    "    #     torch.cat([e, torch.zeros(max_entities - len(e), dtype=torch.long)])\n",
    "    #     for e in entity_ids\n",
    "    # ])\n",
    "\n",
    "    # Padding des binary_labels\n",
    "    # binary_labels = torch.stack([\n",
    "    #     torch.cat([\n",
    "    #         torch.cat([bl, torch.zeros(max_spans - bl.size(0), bl.size(1))], dim=0) if bl.size(0) < max_spans else bl,\n",
    "    #         torch.zeros(max_spans, max_entities - bl.size(1)) if bl.size(1) < max_entities else torch.zeros(0)\n",
    "    #     ], dim=1)\n",
    "    #     for bl in binary_labels\n",
    "    # ])\n",
    "\n",
    "        # Conversion en tensors\n",
    "        #spans = [torch.tensor(s, dtype=torch.long) for s in spans]\n",
    "    input_ids = torch.stack([ids.clone().detach() for ids in input_ids])\n",
    "    attention_masks = torch.stack([mask.clone().detach() for mask in attention_masks]) \n",
    "    entity_ids = torch.stack([e.clone().detach() for e in entity_ids])\n",
    "    binary_labels = torch.stack([bl.clone().detach() for bl in binary_labels])\n",
    "    sentence_masks = torch.stack([sm.clone().detach() for sm in sentence_masks])\n",
    "    entity_masks = torch.stack([em.clone().detach() for em in entity_masks])\n",
    "    spans = torch.tensor([span for span in spans], dtype=torch.long)\n",
    "\n",
    "    return input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 459\n",
      "Validation size: 58\n",
      "Test size: 58\n"
     ]
    }
   ],
   "source": [
    "# Définir les proportions pour le train, validation et test\n",
    "train_ratio = 0.8  # 80% des données pour l'entraînement\n",
    "val_ratio = 0.1    # 10% des données pour la validation\n",
    "test_ratio = 0.1   # 10% des données pour le test\n",
    "\n",
    "# Calculer les tailles des différents ensembles\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = int(val_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Diviser les données en train, validation, et test\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Créer les DataLoaders pour chaque ensemble\n",
    "batch_size = 8  # Ajuster selon vos besoins\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Vérification des tailles\n",
    "print(f\"Train size: {len(train_loader)}\")\n",
    "print(f\"Validation size: {len(val_loader)}\")\n",
    "print(f\"Test size: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class GLiNER(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"microsoft/deberta-v3-base\", span_max_length=2, hidden_size=768, dropout_rate=0.4):\n",
    "        super(GLiNER, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        self.encoder_output_size = self.encoder.config.hidden_size\n",
    "        self.entity_ffn = nn.Sequential(\n",
    "            nn.Linear(self.encoder_output_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.span_ffn = nn.Sequential(\n",
    "            nn.Linear(2 * self.encoder_output_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.span_max_length = span_max_length\n",
    "        # self.loss_fn = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "        pos_weight = torch.tensor([30], dtype=torch.float32)  # Convertir en tenseur si nécessaire\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    def forward(self, input_ids, attention_masks, entity_types, spans, sentence_masks, entity_masks, binary_labels=None):\n",
    "        # print(\"Input IDs shape:\", input_ids.shape)\n",
    "        # print(\"Attention mask shape:\", attention_masks.shape)\n",
    "        # Passer input_ids et attention_masks au modèle\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "    \n",
    "        entity_embeddings, text_embeddings = self.split_embeddings(token_embeddings,len(entity_types[0]))\n",
    "        \n",
    "\n",
    "        refined_entity_embeddings = self.entity_ffn(entity_embeddings)\n",
    "        \n",
    "        span_scores,scores_logit = self.compute_span_scores(refined_entity_embeddings, text_embeddings, spans)\n",
    "\n",
    "        if binary_labels is not None:\n",
    "            #loss = self.compute_loss(span_scores, binary_labels)\n",
    "            loss = self.compute_loss(scores_logit,binary_labels)\n",
    "            return span_scores, loss\n",
    "        \n",
    "        return span_scores\n",
    "\n",
    "\n",
    "    def split_embeddings(self, token_embeddings, num_entity_types = 25):\n",
    "        entity_embeddings = token_embeddings[:, 0:num_entity_types, :]\n",
    "        text_embeddings = token_embeddings[:, num_entity_types + 1:, :]\n",
    "        \n",
    "        return entity_embeddings, text_embeddings\n",
    "\n",
    "    \n",
    "    def compute_span_scores(self, entity_embeddings, text_embeddings, spans):\n",
    "        \"\"\"\n",
    "        Calcule les scores des spans en une seule passe vectorisée, \n",
    "        en supposant que tous les spans sont valides.\n",
    "        \"\"\"\n",
    "        batch_size, text_length, hidden_size = text_embeddings.shape\n",
    "\n",
    "        # Conversion des spans en tensor directement\n",
    "        spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
    "\n",
    "        # Récupération des embeddings des spans\n",
    "        i_indices = spans_tensor[:, :, 0].unsqueeze(-1).expand(-1, -1, hidden_size)  # (batch, num_spans, hidden_size)\n",
    "        j_indices = spans_tensor[:, :, 1].unsqueeze(-1).expand(-1, -1, hidden_size)\n",
    "\n",
    "        start_embeddings = torch.gather(text_embeddings, 1, i_indices)  # (batch, num_spans, hidden_size)\n",
    "        end_embeddings = torch.gather(text_embeddings, 1, j_indices)    # (batch, num_spans, hidden_size)\n",
    "\n",
    "        # Concaténer les embeddings des extrémités et passer dans la FFN\n",
    "        span_reprs = torch.cat([start_embeddings, end_embeddings], dim=-1)  # (batch, num_spans, 2 * hidden_size)\n",
    "        span_reprs = self.span_ffn(span_reprs)                              # (batch, num_spans, hidden_size)\n",
    "\n",
    "        # Calcul des scores pour toutes les entités\n",
    "        scores = torch.einsum(\"bsh,beh->bse\", span_reprs, entity_embeddings)  # (batch, num_spans, num_entity_types)\n",
    "\n",
    "        # Appliquer la sigmoïde pour les scores finaux\n",
    "        span_scores = self.sigmoid(scores)\n",
    "\n",
    "        return span_scores,scores\n",
    "\n",
    "\n",
    "    def compute_loss(self, span_scores, binary_labels):\n",
    "        \"\"\"\n",
    "        Calcul de la perte binaire cross-entropy entre les scores et les étiquettes.\n",
    "        \"\"\"\n",
    "        # print(f\"span_scores shape: {span_scores.shape}\")\n",
    "        # print(f\"binary_labels shape: {binary_labels.shape}\")\n",
    "\n",
    "        # Appliquer la perte\n",
    "        loss = self.loss_fn(span_scores, binary_labels)\n",
    "        return loss\n",
    "\n",
    "    def compute_loss(self, span_scores, binary_labels):\n",
    "        \"\"\"\n",
    "        Calcul de la perte binaire cross-entropy avec des poids pour la classe positive.\n",
    "        \"\"\"\n",
    "        # BCEWithLogitsLoss attend des scores bruts (sans Sigmoid), donc on peut retirer self.sigmoid ici.\n",
    "        loss = self.loss_fn(span_scores, binary_labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/459 [00:00<?, ?batch/s]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_7676\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Training: 100%|██████████| 459/459 [02:58<00:00,  2.57batch/s, Batch Loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5584\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 459/459 [02:58<00:00,  2.57batch/s, Batch Loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2693\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 459/459 [02:58<00:00,  2.57batch/s, Batch Loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2477\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 459/459 [02:59<00:00,  2.56batch/s, Batch Loss=0.344] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2369\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 459/459 [02:59<00:00,  2.56batch/s, Batch Loss=0.228] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparamètres\n",
    "num_epochs = 5\n",
    "# learning_rate = 1e-5\n",
    "\n",
    "# # Optimiseur et scheduler\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "# scheduler = get_scheduler(\n",
    "#     \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * num_epochs\n",
    "# )\n",
    "\n",
    "# Calcul des étapes totales\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "# Optimiseur avec deux taux d'apprentissage\n",
    "optimizer = AdamW([\n",
    "    {'params': model.encoder.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.entity_ffn.parameters(), 'lr': 5e-5},\n",
    "    {'params': model.span_ffn.parameters(), 'lr': 5e-5},\n",
    "])\n",
    "\n",
    "# Scheduler cosinus\n",
    "scheduler = get_scheduler(\n",
    "    \"cosine\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "# Fonction d'entraînement avec tqdm\n",
    "def train_epoch(model, train_loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Ajout de tqdm pour afficher la progression\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        span_scores, loss = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_masks=attention_masks,\n",
    "            entity_types=entity_ids,\n",
    "            spans=spans,\n",
    "            binary_labels=binary_labels,\n",
    "            sentence_masks=sentence_masks,\n",
    "            entity_masks=entity_masks\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Mise à jour de la barre de progression\n",
    "        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# # Boucle d'entraînement avec tqdm\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "#     print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "max_steps = 30000\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    global_step += len(train_loader)\n",
    "    if global_step >= max_steps:\n",
    "        print(\"Reached maximum training steps.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/4588 [00:00<?, ?batch/s]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_24996\\1185905307.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Training:   0%|          | 0/4588 [00:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits\nor torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are\nsafe to autocast.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m     global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "Cell \u001b[1;32mIn[14], line 45\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, optimizer, scheduler, scaler)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Précision mixte avec autocast\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[1;32m---> 45\u001b[0m     span_scores, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mentity_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentity_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbinary_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentence_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentence_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mentity_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentity_masks\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Backpropagation avec le scaler\u001b[39;00m\n\u001b[0;32m     56\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m, in \u001b[0;36mGLiNER.forward\u001b[1;34m(self, input_ids, attention_masks, entity_types, spans, sentence_masks, entity_masks, binary_labels)\u001b[0m\n\u001b[0;32m     43\u001b[0m span_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_span_scores(refined_entity_embeddings, text_embeddings, spans)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m binary_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m span_scores, loss\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m span_scores\n",
      "Cell \u001b[1;32mIn[1], line 97\u001b[0m, in \u001b[0;36mGLiNER.compute_loss\u001b[1;34m(self, span_scores, binary_labels)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03mCalcul de la perte binaire cross-entropy entre les scores et les étiquettes.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# print(f\"span_scores shape: {span_scores.shape}\")\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# print(f\"binary_labels shape: {binary_labels.shape}\")\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Appliquer la perte\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\torch\\nn\\functional.py:3154\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3152\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits\nor torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are\nsafe to autocast."
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.optim import AdamW\n",
    "# from transformers import get_scheduler\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Hyperparamètres\n",
    "# num_epochs = 1\n",
    "# max_steps = 30000\n",
    "# global_step = 0\n",
    "\n",
    "# # Calcul des étapes totales\n",
    "# num_training_steps = len(train_loader) * num_epochs\n",
    "# num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "# # Optimiseur avec deux taux d'apprentissage\n",
    "# optimizer = AdamW([\n",
    "#     {'params': model.encoder.parameters(), 'lr': 1e-5},\n",
    "#     {'params': model.entity_ffn.parameters(), 'lr': 5e-5},\n",
    "#     {'params': model.span_ffn.parameters(), 'lr': 5e-5},\n",
    "# ])\n",
    "\n",
    "# # Scheduler cosinus\n",
    "# scheduler = get_scheduler(\n",
    "#     \"cosine\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    "# )\n",
    "\n",
    "# # Scaler pour la précision mixte\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "# # Fonction d'entraînement avec AMP et tqdm\n",
    "# def train_epoch(model, train_loader, optimizer, scheduler, scaler):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     # Ajout de tqdm pour afficher la progression\n",
    "#     progress_bar = tqdm(train_loader, desc=\"Training\", unit=\"batch\")\n",
    "#     for batch in progress_bar:\n",
    "#         input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks = [b.to(device) for b in batch]\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Précision mixte avec autocast\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             span_scores, loss = model(\n",
    "#                 input_ids=input_ids,\n",
    "#                 attention_masks=attention_masks,\n",
    "#                 entity_types=entity_ids,\n",
    "#                 spans=spans,\n",
    "#                 binary_labels=binary_labels,\n",
    "#                 sentence_masks=sentence_masks,\n",
    "#                 entity_masks=entity_masks\n",
    "#             )\n",
    "\n",
    "#         # Backpropagation avec le scaler\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         scheduler.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Mise à jour de la barre de progression\n",
    "#         progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     return avg_loss\n",
    "\n",
    "\n",
    "# # Boucle d'entraînement avec AMP\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     train_loss = train_epoch(model, train_loader, optimizer, scheduler, scaler)\n",
    "#     print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "#     global_step += len(train_loader)\n",
    "#     if global_step >= max_steps:\n",
    "#         print(\"Reached maximum training steps.\")\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "# Fonction pour calculer la précision, le rappel, le F1-score, l'exactitude et la matrice de confusion\n",
    "def calculate_metrics(binary_scores, binary_labels):\n",
    "    # Déplacer les tensors vers le CPU avant de les convertir en NumPy\n",
    "    binary_scores_flat = binary_scores.cpu().flatten()\n",
    "    binary_labels_flat = binary_labels.cpu().flatten()\n",
    "\n",
    "    # Calcul des métriques\n",
    "    precision = precision_score(binary_labels_flat, binary_scores_flat)\n",
    "    recall = recall_score(binary_labels_flat, binary_scores_flat)\n",
    "    f1 = f1_score(binary_labels_flat, binary_scores_flat)\n",
    "    accuracy = accuracy_score(binary_labels_flat, binary_scores_flat)\n",
    "\n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(binary_labels_flat, binary_scores_flat)\n",
    "\n",
    "    return precision, recall, f1, accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, normalize=False):\n",
    "    # Si normalisation est activée\n",
    "    if normalize:\n",
    "        confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized Confusion Matrix:\")      \n",
    "    else:\n",
    "        # Conversion explicite pour s'assurer d'afficher les entiers\n",
    "        confusion_matrix = confusion_matrix.astype(int)\n",
    "        print(\"Confusion Matrix (Integers):\")\n",
    "\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Normalisation de la matrice de confusion si nécessaire\n",
    "    if normalize:\n",
    "        confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    # Affichage de la matrice de confusion avec format ajusté\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='.4f' if normalize else 'g', cmap='Blues', cbar=False, \n",
    "                xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "                yticklabels=['True Negative', 'True Positive'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, test_loader):\n",
    "    model.eval()  # Passer en mode évaluation\n",
    "    total_loss = 0\n",
    "    test_loss = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_accuracy = 0  # Pour l'accuracy\n",
    "    confusion_matrix_total = torch.zeros(2, 2)  # Confusion matrix for binary classification\n",
    "\n",
    "    progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n",
    "    with torch.no_grad():  # Désactiver les gradients pour la phase de test\n",
    "        for batch in progress_bar:\n",
    "            input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks = [b.to(device) for b in batch]\n",
    "\n",
    "            span_scores, loss = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_masks=attention_masks,\n",
    "                entity_types=entity_ids,\n",
    "                spans=spans,\n",
    "                binary_labels=binary_labels,\n",
    "                sentence_masks=sentence_masks,\n",
    "                entity_masks=entity_masks\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calcul des autres métriques\n",
    "            binary_scores = (span_scores >= 0.5).int()  # Convertir les scores en prédictions binaires\n",
    "            precision, recall, f1, accuracy, cm = calculate_metrics(binary_scores, binary_labels)\n",
    "            \n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "            total_accuracy += accuracy  # Ajout de l'accuracy\n",
    "            confusion_matrix_total += torch.tensor(cm)\n",
    "\n",
    "            # Mise à jour de la barre de progression\n",
    "            progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    # Moyenne des métriques\n",
    "    average_precision = total_precision / len(test_loader)\n",
    "    average_recall = total_recall / len(test_loader)\n",
    "    average_f1 = total_f1 / len(test_loader)\n",
    "    average_accuracy = total_accuracy / len(test_loader)  # Moyenne de l'accuracy\n",
    "    confusion_matrix_total = confusion_matrix_total.numpy()\n",
    "\n",
    "    return avg_loss, average_precision, average_recall, average_f1, average_accuracy, confusion_matrix_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/58 [00:00<?, ?batch/s]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:   2%|▏         | 1/58 [00:00<00:36,  1.58batch/s, Batch Loss=0.202]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:   3%|▎         | 2/58 [00:00<00:25,  2.21batch/s, Batch Loss=0.174]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:   5%|▌         | 3/58 [00:01<00:21,  2.55batch/s, Batch Loss=0.208]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:   7%|▋         | 4/58 [00:01<00:19,  2.75batch/s, Batch Loss=0.174]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:   9%|▊         | 5/58 [00:01<00:18,  2.85batch/s, Batch Loss=0.175]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  10%|█         | 6/58 [00:02<00:17,  2.91batch/s, Batch Loss=0.0733]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  12%|█▏        | 7/58 [00:02<00:17,  2.94batch/s, Batch Loss=0.162] C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  14%|█▍        | 8/58 [00:02<00:17,  2.94batch/s, Batch Loss=0.187]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  16%|█▌        | 9/58 [00:03<00:16,  2.95batch/s, Batch Loss=0.197]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  17%|█▋        | 10/58 [00:03<00:16,  2.96batch/s, Batch Loss=0.149]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  19%|█▉        | 11/58 [00:03<00:15,  2.94batch/s, Batch Loss=0.213]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  21%|██        | 12/58 [00:04<00:15,  2.97batch/s, Batch Loss=0.311]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  22%|██▏       | 13/58 [00:04<00:15,  2.98batch/s, Batch Loss=0.242]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  24%|██▍       | 14/58 [00:04<00:14,  2.96batch/s, Batch Loss=0.347]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  26%|██▌       | 15/58 [00:05<00:14,  2.96batch/s, Batch Loss=0.223]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  28%|██▊       | 16/58 [00:05<00:14,  2.95batch/s, Batch Loss=0.142]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  29%|██▉       | 17/58 [00:05<00:14,  2.92batch/s, Batch Loss=0.201]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  31%|███       | 18/58 [00:06<00:13,  2.93batch/s, Batch Loss=0.276]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  33%|███▎      | 19/58 [00:06<00:13,  2.94batch/s, Batch Loss=0.201]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  34%|███▍      | 20/58 [00:07<00:13,  2.91batch/s, Batch Loss=0.131]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  36%|███▌      | 21/58 [00:07<00:12,  2.91batch/s, Batch Loss=0.135]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  38%|███▊      | 22/58 [00:07<00:12,  2.89batch/s, Batch Loss=0.141]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  40%|███▉      | 23/58 [00:08<00:12,  2.89batch/s, Batch Loss=0.281]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  41%|████▏     | 24/58 [00:08<00:11,  2.88batch/s, Batch Loss=0.168]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  43%|████▎     | 25/58 [00:08<00:11,  2.92batch/s, Batch Loss=0.359]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  45%|████▍     | 26/58 [00:09<00:10,  2.93batch/s, Batch Loss=0.179]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  47%|████▋     | 27/58 [00:09<00:10,  2.93batch/s, Batch Loss=0.284]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  48%|████▊     | 28/58 [00:09<00:10,  2.93batch/s, Batch Loss=0.243]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  50%|█████     | 29/58 [00:10<00:10,  2.88batch/s, Batch Loss=0.244]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  52%|█████▏    | 30/58 [00:10<00:09,  2.90batch/s, Batch Loss=0.196]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  53%|█████▎    | 31/58 [00:10<00:09,  2.90batch/s, Batch Loss=0.318]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  55%|█████▌    | 32/58 [00:11<00:09,  2.89batch/s, Batch Loss=0.12] C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  57%|█████▋    | 33/58 [00:11<00:08,  2.92batch/s, Batch Loss=0.29]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  59%|█████▊    | 34/58 [00:11<00:08,  2.92batch/s, Batch Loss=0.183]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  60%|██████    | 35/58 [00:12<00:07,  2.94batch/s, Batch Loss=0.192]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  62%|██████▏   | 36/58 [00:12<00:07,  2.95batch/s, Batch Loss=0.233]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  64%|██████▍   | 37/58 [00:12<00:07,  2.95batch/s, Batch Loss=0.245]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  66%|██████▌   | 38/58 [00:13<00:06,  2.96batch/s, Batch Loss=0.24] C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  67%|██████▋   | 39/58 [00:13<00:06,  2.96batch/s, Batch Loss=0.203]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  69%|██████▉   | 40/58 [00:13<00:06,  2.91batch/s, Batch Loss=0.245]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  71%|███████   | 41/58 [00:14<00:05,  2.92batch/s, Batch Loss=0.188]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  72%|███████▏  | 42/58 [00:14<00:05,  2.91batch/s, Batch Loss=0.224]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  74%|███████▍  | 43/58 [00:14<00:05,  2.92batch/s, Batch Loss=0.291]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  76%|███████▌  | 44/58 [00:15<00:04,  2.93batch/s, Batch Loss=0.298]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  78%|███████▊  | 45/58 [00:15<00:04,  2.91batch/s, Batch Loss=0.224]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  79%|███████▉  | 46/58 [00:15<00:04,  2.88batch/s, Batch Loss=0.357]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  81%|████████  | 47/58 [00:16<00:03,  2.88batch/s, Batch Loss=0.305]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  83%|████████▎ | 48/58 [00:16<00:03,  2.91batch/s, Batch Loss=0.202]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  84%|████████▍ | 49/58 [00:16<00:03,  2.88batch/s, Batch Loss=0.128]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  86%|████████▌ | 50/58 [00:17<00:02,  2.90batch/s, Batch Loss=0.272]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  88%|████████▊ | 51/58 [00:17<00:02,  2.92batch/s, Batch Loss=0.237]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  90%|████████▉ | 52/58 [00:17<00:02,  2.91batch/s, Batch Loss=0.196]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  91%|█████████▏| 53/58 [00:18<00:01,  2.92batch/s, Batch Loss=0.208]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  93%|█████████▎| 54/58 [00:18<00:01,  2.93batch/s, Batch Loss=0.235]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  95%|█████████▍| 55/58 [00:19<00:01,  2.93batch/s, Batch Loss=0.161]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  97%|█████████▋| 56/58 [00:19<00:00,  2.94batch/s, Batch Loss=0.241]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  98%|█████████▊| 57/58 [00:19<00:00,  2.86batch/s, Batch Loss=0.0962]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing: 100%|██████████| 58/58 [00:19<00:00,  2.91batch/s, Batch Loss=0.212] \n"
     ]
    }
   ],
   "source": [
    "test_loss, test_precision, test_recall, test_f1, test_accuracy, cm = test_epoch(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2166\n",
      "Test Precision: 0.1585\n",
      "Test Recall: 0.9744\n",
      "Test F1 Score: 0.2712\n",
      "Test Accuracy: 0.9253\n",
      "Confusion Matrix (Integers):\n",
      "[[440210  35971]\n",
      " [   163   6656]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "plot_confusion_matrix(cm,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cm[0, 0] = TN\n",
    "- cm[0, 1] = FP\n",
    "- cm[1, 0] = FN\n",
    "- cm[1, 1] = TP\n",
    "\n",
    "[[TN,FP]\n",
    "\n",
    " [FN,TP]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#load \n",
    "model = torch.load(\"model_fullNewBCE2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle complet\n",
    "torch.save(model, \"model_fullNewBCE2.pth\")\n",
    "# Sauvegarder le modèle\n",
    "torch.save(model.state_dict(), \"modelNewBCE2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase d'exemple\n",
    "sentence = \"Alain Farley works at McGill University\"\n",
    "entity_types_to_detect = [\"person\", \"organization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Marie Dupont is a data scientist at OpenAI and she specializes in Python programming.\"\n",
    "entity_types_to_detect = [\"person\", \"organization\",\"programming language\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"John Smith works at Google and has expertise in machine learning and Java.\"\n",
    "entity_types_to_detect = [\"person\", \"organization\", \"programming language\", \"field of expertise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Sundar Pichai is the CEO of Google\"\n",
    "entity_types_to_detect = [\"person\", \"organization\", \"title\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_entity_per_seq = 10\n",
    "max_length = 64\n",
    "max_span_length = 2\n",
    "threshold_score = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_entity_id = []\n",
    "current_entity_str = []\n",
    "\n",
    "for entity_type in entity_types_to_detect:\n",
    "    entity_token_id = model.tokenizer.convert_tokens_to_ids(f'[ENT] {entity_type}')\n",
    "    if entity_token_id not in current_entity_id:\n",
    "        current_entity_id.append(entity_token_id)\n",
    "    if entity_type not in current_entity_str:\n",
    "        current_entity_str.append(entity_type)\n",
    "\n",
    "entity_tokens = \" \".join(f\"[ENT] {et}\" for et in current_entity_str)\n",
    "\n",
    "# Tokeniser la séquence principale\n",
    "encoded = model.tokenizer(\n",
    "    sentence.split(), return_tensors=\"pt\", padding=\"max_length\", truncation=True, \n",
    "    is_split_into_words=True, add_special_tokens=False\n",
    ")\n",
    "\n",
    "word_ids = encoded.word_ids()\n",
    "first_subtoken_ids = [\n",
    "    encoded[\"input_ids\"][0, i].item() for i, word_id in enumerate(word_ids) \n",
    "    if word_id is not None and (i == 0 or word_ids[i - 1] != word_id)\n",
    "]\n",
    "\n",
    "encoded_entity = model.tokenizer(\n",
    "    entity_tokens, return_tensors=\"pt\", padding=\"max_length\", truncation=True, \n",
    "    is_split_into_words=False, add_special_tokens=False\n",
    ")\n",
    "\n",
    "encoded_entity = encoded_entity[\"input_ids\"][0].tolist() + [0]*(max_entity_per_seq-len(current_entity_str))\n",
    "\n",
    "sep_id = model.tokenizer.convert_tokens_to_ids(f'[SEP]')\n",
    "\n",
    "combined_ids = (\n",
    "    encoded_entity +\n",
    "    [sep_id] +\n",
    "    first_subtoken_ids\n",
    ")\n",
    "\n",
    "deleted_ids = max(len(combined_ids) - max_length,0)\n",
    "combined_ids = combined_ids[:max_length]\n",
    "combined_ids += [0] * (max_length - len(combined_ids))\n",
    "\n",
    "# Créer l'attention mask\n",
    "attention_mask = [1 if id != 0 else 0 for id in combined_ids]\n",
    "\n",
    "# Masques spécifiques pour les entités et la phrase\n",
    "entity_mask = [1 if i < len(current_entity_str) else 0 for i in range(len(combined_ids))]\n",
    "sentence_mask = [1 if i > len(encoded_entity) and combined_ids[i] != 0 and combined_ids[i] != sep_id else 0 \n",
    "                    for i in range(len(combined_ids))]\n",
    "\n",
    "current_entity_id = current_entity_id + [0]*(max_entity_per_seq-len(current_entity_str))\n",
    "\n",
    "# Convertir les entités en un tensor\n",
    "entity_tensor = torch.tensor(current_entity_id, dtype=torch.long)\n",
    "\n",
    "# Ajouter les données\n",
    "input_ids_tensor = torch.tensor(combined_ids, dtype=torch.long)\n",
    "attention_mask_tensor = torch.tensor(attention_mask, dtype=torch.long)\n",
    "entity_mask_tensor =torch.tensor(entity_mask, dtype=torch.long)\n",
    "sentence_mask_tensor = torch.tensor(sentence_mask, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in test_loader:\n",
    "#     input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks = [b.to(device) for b in batch]\n",
    "#     input_ids_tensor = input_ids[0]\n",
    "#     attention_mask_tensor = attention_masks[0]\n",
    "#     entity_mask_tensor = entity_masks[0]\n",
    "#     sentence_mask_tensor = sentence_masks[0]\n",
    "#     entity_tensor = entity_ids[0]\n",
    "#     binary_labels = binary_labels\n",
    "#     # Convertir binary_labels[0] en liste et afficher\n",
    "#     binary_labels_list = binary_labels[0].tolist()  # Conversion en liste\n",
    "#     print(binary_labels_list)  # Affichage de la liste\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Entity IDs: [142320, 139751]\n",
      "Decoded Texts: ['[ENT] person', '[ENT] organization']\n",
      "Entity IDs: [142320, 139751, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded Texts: ['[ENT] person', '[ENT] organization']\n"
     ]
    }
   ],
   "source": [
    "# Conversion des IDs en texte\n",
    "entity_ids_list = entity_tensor.tolist()  # Convertir le tenseur en liste\n",
    "entity_types_to_detect = [\n",
    "    model.tokenizer.decode([entity_id], skip_special_tokens=False)\n",
    "    for entity_id in entity_ids_list\n",
    "    if entity_id != 0\n",
    "]\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Filtered Entity IDs:\", [entity_id for entity_id in entity_ids_list if entity_id != 0])\n",
    "print(\"Decoded Texts:\", entity_types_to_detect)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Entity IDs:\", entity_ids_list)\n",
    "print(\"Decoded Texts:\", entity_types_to_detect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(input_ids_tensor) - len(entity_tensor) - 1\n",
    "spans = [\n",
    "    (start, end)\n",
    "    for start in range(num_tokens)\n",
    "    for end in range(start, min(start + max_span_length, num_tokens))\n",
    "]\n",
    "\n",
    "spans_tensor = torch.tensor(spans, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span scores: tensor([[[9.3171e-01, 9.2472e-01, 1.0061e-06,  ..., 1.0061e-06,\n",
      "          1.0061e-06, 1.0061e-06],\n",
      "         [9.3230e-01, 9.2682e-01, 8.2715e-07,  ..., 8.2715e-07,\n",
      "          8.2715e-07, 8.2715e-07],\n",
      "         [9.2269e-01, 9.1526e-01, 5.4937e-07,  ..., 5.4937e-07,\n",
      "          5.4937e-07, 5.4937e-07],\n",
      "         ...,\n",
      "         [2.0884e-04, 1.3943e-04, 8.4146e-12,  ..., 8.4146e-12,\n",
      "          8.4146e-12, 8.4146e-12],\n",
      "         [2.0884e-04, 1.3943e-04, 8.4146e-12,  ..., 8.4146e-12,\n",
      "          8.4146e-12, 8.4146e-12],\n",
      "         [2.0884e-04, 1.3943e-04, 8.4146e-12,  ..., 8.4146e-12,\n",
      "          8.4146e-12, 8.4146e-12]]], device='cuda:0')\n",
      "Binary Span Scores: tensor([[[1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0', dtype=torch.int32)\n",
      "Nombre de 1: 12\n",
      "Masked Binary Span Scores: tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0', dtype=torch.int32)\n",
      "Masked Binary Span Scores: tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_18804\\1884299087.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n"
     ]
    }
   ],
   "source": [
    "# Passage en mode évaluation\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "# Préparation des données de test\n",
    "# Assurez-vous que `input_ids_tensor`, `attention_mask_tensor`, `entity_tensor`, \n",
    "# `entity_mask_tensor`, `sentence_mask_tensor`, et `spans` soient bien définis.\n",
    "\n",
    "with torch.no_grad():  # Pas besoin de calculer les gradients en mode test\n",
    "    span_scores = model(\n",
    "        input_ids=input_ids_tensor.unsqueeze(0).to(device),  # Ajout d'une dimension batch\n",
    "        attention_masks=attention_mask_tensor.unsqueeze(0).to(device),\n",
    "        entity_types=entity_tensor.unsqueeze(0).to(device),\n",
    "        spans=spans_tensor.unsqueeze(0).to(device),\n",
    "        sentence_masks=sentence_mask_tensor.unsqueeze(0).to(device),\n",
    "        entity_masks=entity_mask_tensor.unsqueeze(0).to(device)\n",
    "    )\n",
    "\n",
    "# span_scores contient les scores prédites pour chaque span et chaque entité\n",
    "print(\"Span scores:\", span_scores)\n",
    "\n",
    "# Conversion des span scores en valeurs binaires\n",
    "binary_span_scores = (span_scores > threshold_score).int()\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Binary Span Scores:\", binary_span_scores)\n",
    "\n",
    "# Comptage des valeurs 1 dans tout le tenseur\n",
    "num_ones = binary_span_scores.sum().item()\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Nombre de 1:\", num_ones)\n",
    "\n",
    "# Création d'un masque avec des 0 et 1 représentant la valeur la plus élevée dans chaque liste\n",
    "# Initialisation d'un masque de mêmes dimensions que span_scores\n",
    "max_mask = torch.zeros_like(span_scores, dtype=torch.int)\n",
    "\n",
    "# Parcourir chaque batch, span et entité pour identifier les indices des max\n",
    "for i in range(span_scores.size(1)):  # Dimension des spans\n",
    "    for j in range(span_scores.size(2)):  # Dimension des entités\n",
    "        # Trouver l'indice de la valeur maximale dans la liste\n",
    "        max_index = torch.argmax(span_scores[0, i, :])  # Corrigé pour les dimensions\n",
    "        # Définir 1 à cet indice dans le masque\n",
    "        max_mask[0, i, max_index] = 1  # Corrigé pour les dimensions\n",
    "\n",
    "# Appliquer le masque sur binary_span_scores\n",
    "masked_binary_span_scores = binary_span_scores * max_mask\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Masked Binary Span Scores:\", masked_binary_span_scores)\n",
    "\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Masked Binary Span Scores:\", masked_binary_span_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convertir les tenseurs en numpy\n",
    "# binary_scores_np = binary_span_scores[0].cpu().numpy()\n",
    "# binary_labels_np = binary_labels[0].cpu().numpy()\n",
    "\n",
    "# # Aplatir les matrices pour les rendre 1D\n",
    "# y_pred = binary_scores_np.flatten()  # Prédictions\n",
    "# y_true = binary_labels_np.flatten()  # Labels réels\n",
    "\n",
    "# # Calculer la matrice de confusion\n",
    "# conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# # Afficher la matrice de confusion\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Span Scores with Spans:\n",
      "Example 1:\n",
      "    Span  [ENT] person  [ENT] organization\n",
      "  (0, 0)             1                   0\n",
      "  (0, 1)             1                   0\n",
      "  (1, 1)             1                   0\n",
      "  (1, 2)             0                   0\n",
      "  (2, 2)             0                   0\n",
      "  (2, 3)             0                   0\n",
      "  (3, 3)             0                   0\n",
      "  (3, 4)             0                   0\n",
      "  (4, 4)             1                   0\n",
      "  (4, 5)             1                   0\n",
      "  (5, 5)             0                   1\n",
      "  (5, 6)             0                   0\n",
      "  (6, 6)             0                   0\n",
      "  (6, 7)             0                   0\n",
      "  (7, 7)             0                   0\n",
      "  (7, 8)             0                   0\n",
      "  (8, 8)             0                   0\n",
      "  (8, 9)             0                   0\n",
      "  (9, 9)             0                   0\n",
      " (9, 10)             0                   0\n",
      "(10, 10)             0                   0\n",
      "(10, 11)             0                   0\n",
      "(11, 11)             0                   0\n",
      "(11, 12)             0                   0\n",
      "(12, 12)             0                   0\n",
      "(12, 13)             0                   0\n",
      "(13, 13)             0                   0\n",
      "(13, 14)             0                   0\n",
      "(14, 14)             0                   0\n",
      "(14, 15)             0                   0\n",
      "(15, 15)             0                   0\n",
      "(15, 16)             0                   0\n",
      "(16, 16)             0                   0\n",
      "(16, 17)             0                   0\n",
      "(17, 17)             0                   0\n",
      "(17, 18)             0                   0\n",
      "(18, 18)             0                   0\n",
      "(18, 19)             0                   0\n",
      "(19, 19)             0                   0\n",
      "(19, 20)             0                   0\n",
      "(20, 20)             0                   0\n",
      "(20, 21)             0                   0\n",
      "(21, 21)             0                   0\n",
      "(21, 22)             0                   0\n",
      "(22, 22)             0                   0\n",
      "(22, 23)             0                   0\n",
      "(23, 23)             0                   0\n",
      "(23, 24)             0                   0\n",
      "(24, 24)             0                   0\n",
      "(24, 25)             0                   0\n",
      "(25, 25)             0                   0\n",
      "(25, 26)             0                   0\n",
      "(26, 26)             0                   0\n",
      "(26, 27)             0                   0\n",
      "(27, 27)             0                   0\n",
      "(27, 28)             0                   0\n",
      "(28, 28)             0                   0\n",
      "(28, 29)             0                   0\n",
      "(29, 29)             0                   0\n",
      "(29, 30)             0                   0\n",
      "(30, 30)             0                   0\n",
      "(30, 31)             0                   0\n",
      "(31, 31)             0                   0\n",
      "(31, 32)             0                   0\n",
      "(32, 32)             0                   0\n",
      "(32, 33)             0                   0\n",
      "(33, 33)             0                   0\n",
      "(33, 34)             0                   0\n",
      "(34, 34)             0                   0\n",
      "(34, 35)             0                   0\n",
      "(35, 35)             0                   0\n",
      "(35, 36)             0                   0\n",
      "(36, 36)             0                   0\n",
      "(36, 37)             0                   0\n",
      "(37, 37)             0                   0\n",
      "(37, 38)             0                   0\n",
      "(38, 38)             0                   0\n",
      "(38, 39)             0                   0\n",
      "(39, 39)             0                   0\n",
      "(39, 40)             0                   0\n",
      "(40, 40)             0                   0\n",
      "(40, 41)             0                   0\n",
      "(41, 41)             0                   0\n",
      "(41, 42)             0                   0\n",
      "(42, 42)             0                   0\n",
      "(42, 43)             0                   0\n",
      "(43, 43)             0                   0\n",
      "(43, 44)             0                   0\n",
      "(44, 44)             0                   0\n",
      "(44, 45)             0                   0\n",
      "(45, 45)             0                   0\n",
      "(45, 46)             0                   0\n",
      "(46, 46)             0                   0\n",
      "(46, 47)             0                   0\n",
      "(47, 47)             0                   0\n",
      "(47, 48)             0                   0\n",
      "(48, 48)             0                   0\n",
      "(48, 49)             0                   0\n",
      "(49, 49)             0                   0\n",
      "(49, 50)             0                   0\n",
      "(50, 50)             0                   0\n",
      "(50, 51)             0                   0\n",
      "(51, 51)             0                   0\n",
      "(51, 52)             0                   0\n",
      "(52, 52)             0                   0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Utilisé pour afficher les données en tableau lisible\n",
    "\n",
    "# Conversion en liste pour un affichage plus lisible\n",
    "binary_span_scores_list = masked_binary_span_scores.cpu().numpy().tolist()\n",
    "\n",
    "# Affichage structuré avec les spans associés\n",
    "print(\"Binary Span Scores with Spans:\")\n",
    "for i, example in enumerate(binary_span_scores_list):  # Pour chaque exemple dans le batch\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    \n",
    "    # Création d'une structure tabulaire\n",
    "    table_data = []\n",
    "    for j, span_scores in enumerate(example):  # Pour chaque span dans l'exemple\n",
    "        associated_span = spans[j]  # Associer le span avec l'indice\n",
    "        # Ajout des données du span et des scores associés\n",
    "        table_data.append([associated_span] + span_scores[:len(entity_types_to_detect)])\n",
    "    \n",
    "    # Création d'un DataFrame pour une meilleure lisibilité\n",
    "    columns = [\"Span\"] + entity_types_to_detect  # Colonnes : Span + types d'entités\n",
    "    df = pd.DataFrame(table_data, columns=columns)\n",
    "    print(df.to_string(index=False))  # Affichage sans l'index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "  Span  [ENT] person  [ENT] organization\n",
      "(0, 0)             1                   0\n",
      "(0, 1)             1                   0\n",
      "(1, 1)             1                   0\n",
      "(1, 2)             0                   0\n",
      "(2, 2)             0                   0\n",
      "(2, 3)             0                   0\n",
      "(3, 3)             0                   0\n",
      "(3, 4)             0                   0\n",
      "(4, 4)             1                   0\n",
      "(4, 5)             1                   0\n",
      "(5, 5)             0                   1\n"
     ]
    }
   ],
   "source": [
    "# Longueur limite\n",
    "max_index = len(first_subtoken_ids)\n",
    "\n",
    "# Filtrage et affichage des résultats\n",
    "for i, example in enumerate(binary_span_scores_list):\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    \n",
    "    # Création d'une structure tabulaire\n",
    "    table_data = []\n",
    "    for j, span_scores in enumerate(example):\n",
    "        associated_span = spans[j]\n",
    "        if associated_span[0] < max_index and associated_span[1] < max_index:  # Condition de filtre\n",
    "            table_data.append([associated_span] + span_scores[:len(entity_types_to_detect)])\n",
    "    \n",
    "    # Création du DataFrame\n",
    "    columns = [\"Span\"] + entity_types_to_detect\n",
    "    df = pd.DataFrame(table_data, columns=columns)\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vincentorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

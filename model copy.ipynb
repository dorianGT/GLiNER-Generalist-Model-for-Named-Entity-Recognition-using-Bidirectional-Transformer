{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exemple traité ===\n",
      "Tokenized Text:\n",
      "['Q', ':', 'Position', 'character', 'based', 'on', 'enemy', 'coordinates', 'in', 'lua', 'I', 'have', 'written', 'a', 'function', 'here', 'which', 'should', 'turn', 'my', 'character', 'based', 'on', 'enemy', 'coordinates', 'but', 'it', \"'\", 's', 'not', 'perfect', 'because', 'it', 'does', 'not', 'always', 'turn', 'where', 'I', 'want', 'it', 'to', 'and', 'perhaps', 'there', 'is', 'a', 'better', 'way', 'of', 'writing', 'it', 'local', 'myPosition', '=', '{', 'x', '=', '350', ',', 'y', '=', '355', '}', 'local', 'enemyPosition', '=', '{', 'x', '=', '352', ',', 'y', '=', '354', '}', 'local', 'xValue', ',', 'yValue', ',', 'xDir', ',', 'yDir', ',', 'dir', 'if', 'myPosition', '.', 'x', '>', 'enemyPosition', '.', 'x', 'then', 'xValue', '=', 'myPosition', '.', 'x', '-', 'enemyPosition', '.', 'x', 'elseif', 'myPosition', '.', 'x', '<', 'enemyPosition', '.', 'x', 'then', 'xValue', '=', 'myPosition', '.', 'x', '-', 'enemyPosition', '.', 'x', 'else', 'xValue', '=', '0', 'end', 'if', 'myPosition', '.', 'y', '>', 'enemyPosition', '.', 'y', 'then', 'yValue', '=', 'myPosition', '.', 'y', '-', 'enemyPosition', '.', 'y', 'elseif', 'myPosition', '.', 'y', '<', 'enemyPosition', '.', 'y', 'then', 'yValue', '=', 'myPosition', '.', 'y', '-', 'enemyPosition', '.', 'y', 'else', 'yValue', '=', '0', 'end', 'if', 'xValue', '<', '0', 'then', 'xDir', '=', '\"', 'TURN', 'RIGHT', '\"', 'elseif', 'xValue', '>', '0', 'then', 'xDir', '=', '\"', 'TURN', 'LEFT', '\"', 'end', 'if', 'yValue', '<', '0', 'then', 'yDir', '=', '\"', 'TURN', 'DOWN', '\"', 'elseif', 'yValue', '>', '0', 'then', 'yDir', '=', '\"', 'TURN', 'UP', '\"', 'end', 'if', 'xValue', '>', 'yValue', 'then', 'dir', '=', 'xDir', 'elseif', 'xValue', 'dir', '=', 'yDir', 'end', 'print', '(', '\"', 'Turn', ':', '\"', '.', '.', 'dir', ')', 'And', 'here', 'you', 'have', 'some', 'pictures', 'to', 'further', 'illustrate', 'what', 'I', 'have', 'in', 'mind', ':', 'As', 'you', 'can', 'see', 'on', 'the', 'pictures', ',', 'direction', 'depends', 'on', 'the', 'higher', 'number', '.']\n",
      "\n",
      "NER Spans:\n",
      " - Start: 14, End: 14, Entity Type: programming concept\n",
      " - Start: 9, End: 9, Entity Type: programming language\n",
      " - Start: 53, End: 53, Entity Type: variable\n",
      " - Start: 87, End: 87, Entity Type: variable\n",
      " - Start: 97, End: 97, Entity Type: variable\n",
      " - Start: 105, End: 105, Entity Type: variable\n",
      " - Start: 115, End: 115, Entity Type: variable\n",
      " - Start: 128, End: 128, Entity Type: variable\n",
      " - Start: 138, End: 138, Entity Type: variable\n",
      " - Start: 146, End: 146, Entity Type: variable\n",
      " - Start: 156, End: 156, Entity Type: variable\n",
      " - Start: 65, End: 65, Entity Type: variable\n",
      " - Start: 91, End: 91, Entity Type: variable\n",
      " - Start: 101, End: 101, Entity Type: variable\n",
      " - Start: 109, End: 109, Entity Type: variable\n",
      " - Start: 119, End: 119, Entity Type: variable\n",
      " - Start: 132, End: 132, Entity Type: variable\n",
      " - Start: 142, End: 142, Entity Type: variable\n",
      " - Start: 150, End: 150, Entity Type: variable\n",
      " - Start: 160, End: 160, Entity Type: variable\n",
      " - Start: 77, End: 77, Entity Type: variable\n",
      " - Start: 95, End: 95, Entity Type: variable\n",
      " - Start: 113, End: 113, Entity Type: variable\n",
      " - Start: 123, End: 123, Entity Type: variable\n",
      " - Start: 169, End: 169, Entity Type: variable\n",
      " - Start: 180, End: 180, Entity Type: variable\n",
      " - Start: 215, End: 215, Entity Type: variable\n",
      " - Start: 223, End: 223, Entity Type: variable\n",
      " - Start: 79, End: 79, Entity Type: variable\n",
      " - Start: 136, End: 136, Entity Type: variable\n",
      " - Start: 154, End: 154, Entity Type: variable\n",
      " - Start: 164, End: 164, Entity Type: variable\n",
      " - Start: 192, End: 192, Entity Type: variable\n",
      " - Start: 203, End: 203, Entity Type: variable\n",
      " - Start: 217, End: 217, Entity Type: variable\n",
      " - Start: 81, End: 81, Entity Type: variable\n",
      " - Start: 173, End: 173, Entity Type: variable\n",
      " - Start: 184, End: 184, Entity Type: variable\n",
      " - Start: 221, End: 221, Entity Type: variable\n",
      " - Start: 83, End: 83, Entity Type: variable\n",
      " - Start: 196, End: 196, Entity Type: variable\n",
      " - Start: 207, End: 207, Entity Type: variable\n",
      " - Start: 226, End: 226, Entity Type: variable\n",
      " - Start: 85, End: 85, Entity Type: variable\n",
      " - Start: 219, End: 219, Entity Type: variable\n",
      " - Start: 224, End: 224, Entity Type: variable\n",
      " - Start: 236, End: 236, Entity Type: variable\n",
      "\n",
      "Negative Entities:\n",
      "['database', 'Date']\n"
     ]
    }
   ],
   "source": [
    "# Charger les données traitées pour inspection\n",
    "with open('pilener_train.json', 'r') as f:\n",
    "    processed_data = json.load(f)\n",
    "\n",
    "# Afficher un exemple\n",
    "example_idx = 0  # Modifier cet indice pour voir d'autres exemples\n",
    "example = processed_data[example_idx]\n",
    "\n",
    "# Afficher avec une mise en forme claire\n",
    "print(\"=== Exemple traité ===\")\n",
    "print(\"Tokenized Text:\")\n",
    "print(example['tokenized_text'])\n",
    "print(\"\\nNER Spans:\")\n",
    "for span in example['ner']:\n",
    "    print(f\" - Start: {span[0]}, End: {span[1]}, Entity Type: {span[2]}\")\n",
    "print(\"\\nNegative Entities:\")\n",
    "print(example['negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faut allez à lendroit ou est defini le model puis vous pouvez exec la suite dans lordre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GLiNER(\n",
       "  (encoder): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (entity_ffn): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (span_ffn): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (loss_fn): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GLiNER(pretrained_model_name=\"microsoft/deberta-v3-base\", span_max_length=2, hidden_size=768)\n",
    "# Initialiser le modèle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   0%|          | 0/45889 [00:00<?, ?entry/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing Data:  10%|▉         | 4390/45889 [00:14<02:05, 330.93entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  34%|███▍      | 15547/45889 [00:47<01:29, 337.92entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  38%|███▊      | 17430/45889 [00:53<01:25, 331.91entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  53%|█████▎    | 24230/45889 [01:14<01:05, 332.19entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  64%|██████▍   | 29380/45889 [01:29<00:50, 329.54entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  73%|███████▎  | 33380/45889 [01:41<00:37, 330.41entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  76%|███████▌  | 34898/45889 [01:46<00:32, 334.69entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  88%|████████▊ | 40346/45889 [02:02<00:16, 337.92entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|██████████| 45889/45889 [02:19<00:00, 329.20entry/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def prepare_data_for_training(processed_data, model, max_length=64, max_entity_per_seq = 25):\n",
    "    input_ids, labels, entity_tensors, attention_masks = [], [], [], []\n",
    "    entity_masks, sentence_masks = [], []\n",
    "\n",
    "    # Créer un mapping des types d'entités vers des entiers\n",
    "    entity_types = {entity for entry in processed_data for _, _, entity in entry[\"ner\"]}\n",
    "    special_tokens = [f\"[ENT] {entity}\" for entity in entity_types]\n",
    "    model.tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "    model.encoder.resize_token_embeddings(len(model.tokenizer))\n",
    "\n",
    "    for entry in tqdm(processed_data, desc=\"Processing Data\", unit=\"entry\"):\n",
    "        tokenized_text = entry[\"tokenized_text\"]\n",
    "        ner_spans = entry[\"ner\"]\n",
    "                \n",
    "        # Générer le tensor de labels\n",
    "        label_tensor = torch.zeros(max_length, dtype=torch.long)\n",
    "        current_entity_id = []\n",
    "        current_entity_str = []\n",
    "\n",
    "        for start, end, entity_type in ner_spans:\n",
    "            if start < max_length and end < max_length and len(current_entity_str)<max_entity_per_seq:\n",
    "                entity_token_id = model.tokenizer.convert_tokens_to_ids(f'[ENT] {entity_type}')\n",
    "                label_tensor[start:end + 1] = entity_token_id\n",
    "\n",
    "                if entity_token_id not in current_entity_id:\n",
    "                    current_entity_id.append(entity_token_id)\n",
    "                if entity_type not in current_entity_str:\n",
    "                    current_entity_str.append(entity_type)\n",
    "\n",
    "        entity_tokens = \" \".join(f\"[ENT] {et}\" for et in current_entity_str)\n",
    "        \n",
    "        # Tokeniser la séquence principale\n",
    "        encoded = model.tokenizer(\n",
    "            tokenized_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, \n",
    "            is_split_into_words=True, add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        word_ids = encoded.word_ids()\n",
    "        first_subtoken_ids = [\n",
    "            encoded[\"input_ids\"][0, i].item() for i, word_id in enumerate(word_ids) \n",
    "            if word_id is not None and (i == 0 or word_ids[i - 1] != word_id)\n",
    "        ]\n",
    "\n",
    "        encoded_entity = model.tokenizer(\n",
    "            entity_tokens, return_tensors=\"pt\", padding=\"max_length\", truncation=True, \n",
    "            is_split_into_words=False, add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        if len(current_entity_str) != len(encoded_entity[\"input_ids\"][0]) or len(tokenized_text) != len(first_subtoken_ids):\n",
    "            print(\"error, not same size\")\n",
    "            continue\n",
    "\n",
    "        encoded_entity = encoded_entity[\"input_ids\"][0].tolist() + [0]*(max_entity_per_seq-len(current_entity_str))\n",
    "\n",
    "        sep_id = model.tokenizer.convert_tokens_to_ids(f'[SEP]')\n",
    "\n",
    "        combined_ids = (\n",
    "            encoded_entity +\n",
    "            [sep_id] +\n",
    "            first_subtoken_ids\n",
    "        )\n",
    "\n",
    "        if len(combined_ids) != max_entity_per_seq + len(first_subtoken_ids) + 1:\n",
    "            print(\"error, not same size\")\n",
    "            continue\n",
    "\n",
    "        deleted_ids = max(len(combined_ids) - max_length,0)\n",
    "        combined_ids = combined_ids[:max_length]\n",
    "        combined_ids += [0] * (max_length - len(combined_ids))\n",
    "\n",
    "        # Créer l'attention mask\n",
    "        attention_mask = [1 if id != 0 else 0 for id in combined_ids]\n",
    "\n",
    "        # Masques spécifiques pour les entités et la phrase\n",
    "        entity_mask = [1 if i < len(current_entity_str) else 0 for i in range(len(combined_ids))]\n",
    "        sentence_mask = [1 if i > len(encoded_entity) and combined_ids[i] != 0 and combined_ids[i] != sep_id else 0 \n",
    "                         for i in range(len(combined_ids))]\n",
    "\n",
    "        # Vérification des tailles\n",
    "        if sum(entity_mask) != len(current_entity_str):\n",
    "            print(f\"Entity mask size mismatch: {sum(entity_mask)} != {len(current_entity_str)}\")\n",
    "            continue\n",
    "        if sum(sentence_mask) != len(tokenized_text)-deleted_ids:\n",
    "            print(f\"Sentence mask size mismatch: {sum(sentence_mask)} != {len(tokenized_text)-deleted_ids}\")\n",
    "            continue\n",
    "\n",
    "        current_entity_id = current_entity_id + [0]*(max_entity_per_seq-len(current_entity_str))\n",
    "\n",
    "        # Convertir les entités en un tensor\n",
    "        entity_tensor = torch.tensor(current_entity_id, dtype=torch.long)\n",
    "        \n",
    "        # Ajouter les données\n",
    "        input_ids.append(torch.tensor(combined_ids, dtype=torch.long))\n",
    "        labels.append(label_tensor)\n",
    "        entity_tensors.append(entity_tensor)\n",
    "        attention_masks.append(torch.tensor(attention_mask, dtype=torch.long))\n",
    "        entity_masks.append(torch.tensor(entity_mask, dtype=torch.long))\n",
    "        sentence_masks.append(torch.tensor(sentence_mask, dtype=torch.long))\n",
    "\n",
    "    return (\n",
    "        torch.stack(input_ids), \n",
    "        torch.stack(labels), \n",
    "        entity_tensors, \n",
    "        torch.stack(attention_masks), \n",
    "        torch.stack(entity_masks), \n",
    "        torch.stack(sentence_masks)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Charger les données générées précédemment\n",
    "with open('pilener_train.json', 'r') as f:\n",
    "    processed_data = json.load(f)\n",
    "\n",
    "# Préparer les données avec suivi d'avancement\n",
    "input_ids, labels, entity_tensors, attention_masks, entity_masks, sentence_masks = prepare_data_for_training(processed_data, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143177\n",
      "45881\n",
      "45881\n"
     ]
    }
   ],
   "source": [
    "print(len(model.tokenizer))\n",
    "print(len(input_ids))\n",
    "print(len(entity_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de input_ids : torch.Size([45881, 64])\n",
      "Forme de attention_masks : torch.Size([45881, 64])\n",
      "Forme de labels : torch.Size([45881, 64])\n",
      "Forme de entity_masks : torch.Size([45881, 64])\n",
      "Forme de sentence_masks : torch.Size([45881, 64])\n",
      "\n",
      "Exemple de input_ids (première entrée) :\n",
      "tensor([130368, 128249, 138174,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      2,   1729,\n",
      "           877,  18172,   1470,    636,    277,   4648,  14321,    267,  96792,\n",
      "           273,    286,   1223,    266,   1571,    422,    319,    403,    930,\n",
      "           312,   1470,    636,    277,   4648,  14321,    304,    278,    382,\n",
      "          1550,    298,    801,    401,    278,    490,    298,    489,    930,\n",
      "           399])\n",
      "\n",
      "Exemple de attention_masks (première entrée) :\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "Exemple de labels (première entrée) :\n",
      "tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "        128249,      0,      0,      0,      0, 130368,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0, 138174,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0])\n",
      "\n",
      "Exemple de entity_tensors (première entrée) :\n",
      "tensor([130368, 128249, 138174,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0])\n",
      "\n",
      "Exemple de entity_masks (première entrée) :\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "Exemple de sentence_masks (première entrée) :\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Afficher les formes des tensors principaux\n",
    "print(f\"Forme de input_ids : {input_ids.shape}\")\n",
    "print(f\"Forme de attention_masks : {attention_masks.shape}\")\n",
    "print(f\"Forme de labels : {labels.shape}\")\n",
    "print(f\"Forme de entity_masks : {entity_masks.shape}\")\n",
    "print(f\"Forme de sentence_masks : {sentence_masks.shape}\")\n",
    "\n",
    "# Afficher un exemple pour les tensors principaux\n",
    "print(\"\\nExemple de input_ids (première entrée) :\")\n",
    "print(input_ids[0])\n",
    "\n",
    "print(\"\\nExemple de attention_masks (première entrée) :\")\n",
    "print(attention_masks[0])\n",
    "\n",
    "print(\"\\nExemple de labels (première entrée) :\")\n",
    "print(labels[0])\n",
    "\n",
    "# Afficher un exemple pour les entity_tensors\n",
    "print(\"\\nExemple de entity_tensors (première entrée) :\")\n",
    "print(entity_tensors[0])\n",
    "\n",
    "# Afficher un exemple pour les entity_masks\n",
    "print(\"\\nExemple de entity_masks (première entrée) :\")\n",
    "print(entity_masks[0])\n",
    "\n",
    "# Afficher un exemple pour les sentence_masks\n",
    "print(\"\\nExemple de sentence_masks (première entrée) :\")\n",
    "print(sentence_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels, entities,entity_masks,sentence_masks , max_span_length=2):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.entity_masks = entity_masks\n",
    "        self.sentence_masks =sentence_masks\n",
    "        self.labels = labels  # Liste des labels pour chaque token\n",
    "        self.entities = entities  # Liste des entités uniques\n",
    "        self.max_span_length = max_span_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_id = self.input_ids[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        token_labels = self.labels[idx]  # Labels token-par-token\n",
    "        entity_ids = self.entities[idx]  # Entités pour cet exemple\n",
    "\n",
    "        num_tokens = len(input_id) - len(entity_ids) - 1\n",
    "        spans = [\n",
    "            (start, end)\n",
    "            for start in range(num_tokens)\n",
    "            for end in range(start, min(start + self.max_span_length, num_tokens))\n",
    "        ]\n",
    "        num_spans = len(spans)\n",
    "        num_entities = len(entity_ids)\n",
    "\n",
    "        # Matrice binaire : spans x entities\n",
    "        binary_labels = torch.zeros(num_spans, num_entities, dtype=torch.float)\n",
    "\n",
    "        for span_idx, (start, end) in enumerate(spans):\n",
    "            span_labels = token_labels[start:end + 1]\n",
    "            for entity_idx, entity_id in enumerate(entity_ids):\n",
    "                if all((label == entity_id and label != 0)  for label in span_labels):\n",
    "                    binary_labels[span_idx, entity_idx] = 1\n",
    "\n",
    "        return input_id, attention_mask, spans, entity_ids, binary_labels, sentence_masks, entity_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = int(len(input_ids) * proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID: tensor([130368, 128249, 138174,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      2,   1729,\n",
      "           877,  18172,   1470,    636,    277,   4648,  14321,    267,  96792,\n",
      "           273,    286,   1223,    266,   1571,    422,    319,    403,    930,\n",
      "           312,   1470,    636,    277,   4648,  14321,    304,    278,    382,\n",
      "          1550,    298,    801,    401,    278,    490,    298,    489,    930,\n",
      "           399])\n",
      "Spans: [(0, 0), (0, 1), (1, 1), (1, 2), (2, 2), (2, 3), (3, 3), (3, 4), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (6, 7), (7, 7), (7, 8), (8, 8), (8, 9), (9, 9), (9, 10), (10, 10), (10, 11), (11, 11), (11, 12), (12, 12), (12, 13), (13, 13), (13, 14), (14, 14), (14, 15), (15, 15), (15, 16), (16, 16), (16, 17), (17, 17), (17, 18), (18, 18), (18, 19), (19, 19), (19, 20), (20, 20), (20, 21), (21, 21), (21, 22), (22, 22), (22, 23), (23, 23), (23, 24), (24, 24), (24, 25), (25, 25), (25, 26), (26, 26), (26, 27), (27, 27), (27, 28), (28, 28), (28, 29), (29, 29), (29, 30), (30, 30), (30, 31), (31, 31), (31, 32), (32, 32), (32, 33), (33, 33), (33, 34), (34, 34), (34, 35), (35, 35), (35, 36), (36, 36), (36, 37), (37, 37)]\n",
      "Entity IDs: tensor([130368, 128249, 138174,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0])\n",
      "Binary Labels: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Créer le dataset\n",
    "dataset = NERDataset(input_ids[:ind], attention_masks[:ind], labels[:ind], entity_tensors[:ind],entity_masks[:ind],sentence_masks[:ind])\n",
    "\n",
    "# Exemple de récupération d'une entrée\n",
    "input_id, attention_mask, spans, entity_ids, binary_labels, sentence_masks, entity_masks = dataset[0]\n",
    "\n",
    "print(\"Input ID:\", input_id)\n",
    "print(\"Spans:\", spans)\n",
    "print(\"Entity IDs:\", entity_ids)\n",
    "print(\"Binary Labels:\", binary_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks = zip(*batch)\n",
    "\n",
    "    # Trouver les tailles maximales pour le padding\n",
    "    max_len = max(len(ids) for ids in input_ids)  # Longueur max des tokens\n",
    "    max_spans = max(len(s) for s in spans)  # Nombre max de spans\n",
    "    max_entities = max(len(e) for e in entity_ids)  # Nombre max d'entités\n",
    "\n",
    "    # Padding des input_ids et attention_masks\n",
    "    # padded_input_ids = torch.stack([\n",
    "    #     torch.cat([ids, torch.zeros(max_len - len(ids), dtype=torch.long)])\n",
    "    #     for ids in input_ids\n",
    "    # ])\n",
    "    # padded_attention_masks = torch.stack([\n",
    "    #     torch.cat([mask, torch.zeros(max_len - len(mask), dtype=torch.long)])\n",
    "    #     for mask in attention_masks\n",
    "    # ])\n",
    "\n",
    "    # Padding des spans\n",
    "    # spans = torch.stack([\n",
    "    #     torch.cat([torch.tensor(s, dtype=torch.long), torch.zeros((max_spans - len(s), 2), dtype=torch.long)])\n",
    "    #     for s in spans\n",
    "    # ])\n",
    "\n",
    "    # Padding des entity_ids\n",
    "    # padded_entity_ids = torch.stack([\n",
    "    #     torch.cat([e, torch.zeros(max_entities - len(e), dtype=torch.long)])\n",
    "    #     for e in entity_ids\n",
    "    # ])\n",
    "\n",
    "    # Padding des binary_labels\n",
    "    # binary_labels = torch.stack([\n",
    "    #     torch.cat([\n",
    "    #         torch.cat([bl, torch.zeros(max_spans - bl.size(0), bl.size(1))], dim=0) if bl.size(0) < max_spans else bl,\n",
    "    #         torch.zeros(max_spans, max_entities - bl.size(1)) if bl.size(1) < max_entities else torch.zeros(0)\n",
    "    #     ], dim=1)\n",
    "    #     for bl in binary_labels\n",
    "    # ])\n",
    "\n",
    "        # Conversion en tensors\n",
    "        #spans = [torch.tensor(s, dtype=torch.long) for s in spans]\n",
    "    input_ids = torch.stack([ids.clone().detach() for ids in input_ids])\n",
    "    attention_masks = torch.stack([mask.clone().detach() for mask in attention_masks]) \n",
    "    entity_ids = torch.stack([e.clone().detach() for e in entity_ids])\n",
    "    binary_labels = torch.stack([bl.clone().detach() for bl in binary_labels])\n",
    "    sentence_masks = torch.stack([sm.clone().detach() for sm in sentence_masks])\n",
    "    entity_masks = torch.stack([em.clone().detach() for em in entity_masks])\n",
    "    spans = torch.tensor([span for span in spans], dtype=torch.long)\n",
    "\n",
    "    return input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 230\n",
      "Validation size: 29\n",
      "Test size: 29\n"
     ]
    }
   ],
   "source": [
    "# Définir les proportions pour le train, validation et test\n",
    "train_ratio = 0.8  # 80% des données pour l'entraînement\n",
    "val_ratio = 0.1    # 10% des données pour la validation\n",
    "test_ratio = 0.1   # 10% des données pour le test\n",
    "\n",
    "# Calculer les tailles des différents ensembles\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = int(val_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Diviser les données en train, validation, et test\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Créer les DataLoaders pour chaque ensemble\n",
    "batch_size = 16  # Ajuster selon vos besoins\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Vérification des tailles\n",
    "print(f\"Train size: {len(train_loader)}\")\n",
    "print(f\"Validation size: {len(val_loader)}\")\n",
    "print(f\"Test size: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class GLiNER(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"microsoft/deberta-v3-base\", span_max_length=2, hidden_size=768):\n",
    "        super(GLiNER, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        self.entity_ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.span_ffn = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.span_max_length = span_max_length\n",
    "        self.loss_fn = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "    def forward(self, input_ids, attention_masks, entity_types, spans, sentence_masks, entity_masks, binary_labels=None):\n",
    "        # print(\"Input IDs shape:\", input_ids.shape)\n",
    "        # print(\"Attention mask shape:\", attention_masks.shape)\n",
    "        # Passer input_ids et attention_masks au modèle\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "    \n",
    "        entity_embeddings, text_embeddings = self.split_embeddings(token_embeddings,len(entity_types[0]))\n",
    "        \n",
    "\n",
    "        refined_entity_embeddings = self.entity_ffn(entity_embeddings)\n",
    "        \n",
    "        span_scores = self.compute_span_scores(refined_entity_embeddings, text_embeddings, spans)\n",
    "\n",
    "        if binary_labels is not None:\n",
    "            loss = self.compute_loss(span_scores, binary_labels)\n",
    "            return span_scores, loss\n",
    "        \n",
    "        return span_scores\n",
    "\n",
    "\n",
    "    def split_embeddings(self, token_embeddings, num_entity_types = 25):\n",
    "        entity_embeddings = token_embeddings[:, 0:num_entity_types, :]\n",
    "        text_embeddings = token_embeddings[:, num_entity_types + 1:, :]\n",
    "        \n",
    "        return entity_embeddings, text_embeddings\n",
    "\n",
    "    \n",
    "    def compute_span_scores(self, entity_embeddings, text_embeddings, spans):\n",
    "        \"\"\"\n",
    "        Calcule les scores des spans en une seule passe vectorisée, \n",
    "        en supposant que tous les spans sont valides.\n",
    "        \"\"\"\n",
    "        batch_size, text_length, hidden_size = text_embeddings.shape\n",
    "\n",
    "        # Conversion des spans en tensor directement\n",
    "        spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
    "\n",
    "        # Récupération des embeddings des spans\n",
    "        i_indices = spans_tensor[:, :, 0].unsqueeze(-1).expand(-1, -1, hidden_size)  # (batch, num_spans, hidden_size)\n",
    "        j_indices = spans_tensor[:, :, 1].unsqueeze(-1).expand(-1, -1, hidden_size)\n",
    "\n",
    "        start_embeddings = torch.gather(text_embeddings, 1, i_indices)  # (batch, num_spans, hidden_size)\n",
    "        end_embeddings = torch.gather(text_embeddings, 1, j_indices)    # (batch, num_spans, hidden_size)\n",
    "\n",
    "        # Concaténer les embeddings des extrémités et passer dans la FFN\n",
    "        span_reprs = torch.cat([start_embeddings, end_embeddings], dim=-1)  # (batch, num_spans, 2 * hidden_size)\n",
    "        span_reprs = self.span_ffn(span_reprs)                              # (batch, num_spans, hidden_size)\n",
    "\n",
    "        # Calcul des scores pour toutes les entités\n",
    "        scores = torch.einsum(\"bsh,beh->bse\", span_reprs, entity_embeddings)  # (batch, num_spans, num_entity_types)\n",
    "\n",
    "        # Appliquer la sigmoïde pour les scores finaux\n",
    "        span_scores = self.sigmoid(scores)\n",
    "\n",
    "        return span_scores\n",
    "\n",
    "\n",
    "    def compute_loss(self, span_scores, binary_labels):\n",
    "        \"\"\"\n",
    "        Calcul de la perte binaire cross-entropy entre les scores et les étiquettes.\n",
    "        \"\"\"\n",
    "        # print(f\"span_scores shape: {span_scores.shape}\")\n",
    "        # print(f\"binary_labels shape: {binary_labels.shape}\")\n",
    "\n",
    "        # Appliquer la perte\n",
    "        loss = self.loss_fn(span_scores, binary_labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/230 [00:00<?, ?batch/s]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Training: 100%|██████████| 230/230 [03:26<00:00,  1.11batch/s, Batch Loss=0.0243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0364\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 230/230 [03:25<00:00,  1.12batch/s, Batch Loss=0.0197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0201\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 230/230 [03:25<00:00,  1.12batch/s, Batch Loss=0.0199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0188\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 230/230 [03:26<00:00,  1.11batch/s, Batch Loss=0.0193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0183\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 230/230 [03:25<00:00,  1.12batch/s, Batch Loss=0.0202] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparamètres\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Optimiseur et scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * num_epochs\n",
    ")\n",
    "\n",
    "# Fonction d'entraînement avec tqdm\n",
    "def train_epoch(model, train_loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Ajout de tqdm pour afficher la progression\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        span_scores, loss = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_masks=attention_masks,\n",
    "            entity_types=entity_ids,\n",
    "            spans=spans,\n",
    "            binary_labels=binary_labels,\n",
    "            sentence_masks=sentence_masks,\n",
    "            entity_masks=entity_masks\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Mise à jour de la barre de progression\n",
    "        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Boucle d'entraînement avec tqdm\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Fonction pour calculer la précision, le rappel, le F1-score et la matrice de confusion\n",
    "def calculate_metrics(binary_scores, binary_labels):\n",
    "    # Déplacer les tensors vers le CPU avant de les convertir en NumPy\n",
    "    binary_scores_flat = binary_scores.cpu().flatten()\n",
    "    binary_labels_flat = binary_labels.cpu().flatten()\n",
    "\n",
    "    # Calcul des métriques\n",
    "    precision = precision_score(binary_labels_flat, binary_scores_flat)\n",
    "    recall = recall_score(binary_labels_flat, binary_scores_flat)\n",
    "    f1 = f1_score(binary_labels_flat, binary_scores_flat)\n",
    "\n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(binary_labels_flat, binary_scores_flat)\n",
    "\n",
    "    return precision, recall, f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, normalize=False):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    # Normalisation de la matrice de confusion si nécessaire\n",
    "    if normalize:\n",
    "        confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    print(confusion_matrix)\n",
    "    # Affichage de la matrice de confusion avec format ajusté\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='.4f' if normalize else 'g', cmap='Blues', cbar=False, \n",
    "                xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "                yticklabels=['True Negative', 'True Positive'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, test_loader):\n",
    "    model.eval()  # Passer en mode évaluation\n",
    "    total_loss = 0\n",
    "    test_loss = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    confusion_matrix_total = torch.zeros(2, 2)  # Confusion matrix for binary classification\n",
    "\n",
    "    progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n",
    "    with torch.no_grad():  # Désactiver les gradients pour la phase de test\n",
    "        for batch in progress_bar:\n",
    "            input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks = [b.to(device) for b in batch]\n",
    "\n",
    "            span_scores, loss = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_masks=attention_masks,\n",
    "                entity_types=entity_ids,\n",
    "                spans=spans,\n",
    "                binary_labels=binary_labels,\n",
    "                sentence_masks=sentence_masks,\n",
    "                entity_masks=entity_masks\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calcul des autres métriques\n",
    "            binary_scores = (span_scores >= 0.5).float()  # Convertir les scores en prédictions binaires\n",
    "            precision, recall, f1, cm = calculate_metrics(binary_scores, binary_labels)\n",
    "            \n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "            confusion_matrix_total += torch.tensor(cm)\n",
    "\n",
    "            # Mise à jour de la barre de progression\n",
    "            progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    # Moyenne des métriques\n",
    "    average_precision = total_precision / len(test_loader)\n",
    "    average_recall = total_recall / len(test_loader)\n",
    "    average_f1 = total_f1 / len(test_loader)\n",
    "    confusion_matrix_total = confusion_matrix_total.numpy()\n",
    "\n",
    "    return avg_loss, average_precision,average_recall,average_f1,confusion_matrix_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/29 [00:00<?, ?batch/s]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:   3%|▎         | 1/29 [00:00<00:24,  1.13batch/s, Batch Loss=0.0181]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:   7%|▋         | 2/29 [00:01<00:22,  1.21batch/s, Batch Loss=0.0161]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  10%|█         | 3/29 [00:02<00:21,  1.23batch/s, Batch Loss=0.0127]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  14%|█▍        | 4/29 [00:03<00:19,  1.25batch/s, Batch Loss=0.0156]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  17%|█▋        | 5/29 [00:04<00:19,  1.25batch/s, Batch Loss=0.0164]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  21%|██        | 6/29 [00:04<00:18,  1.25batch/s, Batch Loss=0.0146]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  24%|██▍       | 7/29 [00:05<00:17,  1.26batch/s, Batch Loss=0.0202]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  28%|██▊       | 8/29 [00:06<00:16,  1.26batch/s, Batch Loss=0.0149]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  31%|███       | 9/29 [00:07<00:15,  1.26batch/s, Batch Loss=0.019] C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  34%|███▍      | 10/29 [00:08<00:15,  1.26batch/s, Batch Loss=0.015]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  38%|███▊      | 11/29 [00:08<00:14,  1.27batch/s, Batch Loss=0.0157]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  41%|████▏     | 12/29 [00:09<00:13,  1.26batch/s, Batch Loss=0.0153]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  45%|████▍     | 13/29 [00:10<00:12,  1.26batch/s, Batch Loss=0.0142]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  48%|████▊     | 14/29 [00:11<00:11,  1.26batch/s, Batch Loss=0.0179]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  52%|█████▏    | 15/29 [00:11<00:11,  1.26batch/s, Batch Loss=0.0134]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  55%|█████▌    | 16/29 [00:12<00:10,  1.26batch/s, Batch Loss=0.0168]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  59%|█████▊    | 17/29 [00:13<00:09,  1.26batch/s, Batch Loss=0.0116]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  62%|██████▏   | 18/29 [00:14<00:08,  1.26batch/s, Batch Loss=0.019] C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  66%|██████▌   | 19/29 [00:15<00:07,  1.27batch/s, Batch Loss=0.022]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  69%|██████▉   | 20/29 [00:15<00:07,  1.27batch/s, Batch Loss=0.0205]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  72%|███████▏  | 21/29 [00:16<00:06,  1.26batch/s, Batch Loss=0.0182]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  76%|███████▌  | 22/29 [00:17<00:05,  1.24batch/s, Batch Loss=0.0167]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  79%|███████▉  | 23/29 [00:18<00:04,  1.24batch/s, Batch Loss=0.0171]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  83%|████████▎ | 24/29 [00:19<00:04,  1.24batch/s, Batch Loss=0.0191]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  86%|████████▌ | 25/29 [00:19<00:03,  1.25batch/s, Batch Loss=0.0173]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  90%|████████▉ | 26/29 [00:20<00:02,  1.25batch/s, Batch Loss=0.0199]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  93%|█████████▎| 27/29 [00:21<00:01,  1.26batch/s, Batch Loss=0.0205]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  97%|█████████▋| 28/29 [00:22<00:00,  1.27batch/s, Batch Loss=0.0193]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_32132\\3357513992.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing: 100%|██████████| 29/29 [00:22<00:00,  1.26batch/s, Batch Loss=0.0155]\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_precision, test_recall, test_f1, cm = test_epoch(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0170\n",
      "Test Precision: 0.4807\n",
      "Test Recall: 0.0763\n",
      "Test F1 Score: 0.1264\n",
      "[[9.99473907e-01 5.26092926e-04]\n",
      " [9.23620393e-01 7.63796066e-02]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "plot_confusion_matrix(cm,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cm[0, 0] = TN\n",
    "- cm[0, 1] = FP\n",
    "- cm[1, 0] = FN\n",
    "- cm[1, 1] = TP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.GLiNER'>: it's not the same object as __main__.GLiNER",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sauvegarder le modèle complet\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_fullBest.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Sauvegarder le modèle\u001b[39;00m\n\u001b[0;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelBest.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\torch\\serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 628\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\torch\\serialization.py:840\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    838\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[0;32m    839\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[1;32m--> 840\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    842\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <class '__main__.GLiNER'>: it's not the same object as __main__.GLiNER"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le modèle complet\n",
    "torch.save(model, \"model_fullBest.pth\")\n",
    "# Sauvegarder le modèle\n",
    "torch.save(model.state_dict(), \"modelBest.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vincentorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

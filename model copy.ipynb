{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exemple traité ===\n",
      "Tokenized Text:\n",
      "['Q', ':', 'Position', 'character', 'based', 'on', 'enemy', 'coordinates', 'in', 'lua', 'I', 'have', 'written', 'a', 'function', 'here', 'which', 'should', 'turn', 'my', 'character', 'based', 'on', 'enemy', 'coordinates', 'but', 'it', \"'\", 's', 'not', 'perfect', 'because', 'it', 'does', 'not', 'always', 'turn', 'where', 'I', 'want', 'it', 'to', 'and', 'perhaps', 'there', 'is', 'a', 'better', 'way', 'of', 'writing', 'it', 'local', 'myPosition', '=', '{', 'x', '=', '350', ',', 'y', '=', '355', '}', 'local', 'enemyPosition', '=', '{', 'x', '=', '352', ',', 'y', '=', '354', '}', 'local', 'xValue', ',', 'yValue', ',', 'xDir', ',', 'yDir', ',', 'dir', 'if', 'myPosition', '.', 'x', '>', 'enemyPosition', '.', 'x', 'then', 'xValue', '=', 'myPosition', '.', 'x', '-', 'enemyPosition', '.', 'x', 'elseif', 'myPosition', '.', 'x', '<', 'enemyPosition', '.', 'x', 'then', 'xValue', '=', 'myPosition', '.', 'x', '-', 'enemyPosition', '.', 'x', 'else', 'xValue', '=', '0', 'end', 'if', 'myPosition', '.', 'y', '>', 'enemyPosition', '.', 'y', 'then', 'yValue', '=', 'myPosition', '.', 'y', '-', 'enemyPosition', '.', 'y', 'elseif', 'myPosition', '.', 'y', '<', 'enemyPosition', '.', 'y', 'then', 'yValue', '=', 'myPosition', '.', 'y', '-', 'enemyPosition', '.', 'y', 'else', 'yValue', '=', '0', 'end', 'if', 'xValue', '<', '0', 'then', 'xDir', '=', '\"', 'TURN', 'RIGHT', '\"', 'elseif', 'xValue', '>', '0', 'then', 'xDir', '=', '\"', 'TURN', 'LEFT', '\"', 'end', 'if', 'yValue', '<', '0', 'then', 'yDir', '=', '\"', 'TURN', 'DOWN', '\"', 'elseif', 'yValue', '>', '0', 'then', 'yDir', '=', '\"', 'TURN', 'UP', '\"', 'end', 'if', 'xValue', '>', 'yValue', 'then', 'dir', '=', 'xDir', 'elseif', 'xValue', 'dir', '=', 'yDir', 'end', 'print', '(', '\"', 'Turn', ':', '\"', '.', '.', 'dir', ')', 'And', 'here', 'you', 'have', 'some', 'pictures', 'to', 'further', 'illustrate', 'what', 'I', 'have', 'in', 'mind', ':', 'As', 'you', 'can', 'see', 'on', 'the', 'pictures', ',', 'direction', 'depends', 'on', 'the', 'higher', 'number', '.']\n",
      "\n",
      "NER Spans:\n",
      " - Start: 14, End: 14, Entity Type: programming concept\n",
      " - Start: 9, End: 9, Entity Type: programming language\n",
      " - Start: 53, End: 53, Entity Type: variable\n",
      " - Start: 87, End: 87, Entity Type: variable\n",
      " - Start: 97, End: 97, Entity Type: variable\n",
      " - Start: 105, End: 105, Entity Type: variable\n",
      " - Start: 115, End: 115, Entity Type: variable\n",
      " - Start: 128, End: 128, Entity Type: variable\n",
      " - Start: 138, End: 138, Entity Type: variable\n",
      " - Start: 146, End: 146, Entity Type: variable\n",
      " - Start: 156, End: 156, Entity Type: variable\n",
      " - Start: 65, End: 65, Entity Type: variable\n",
      " - Start: 91, End: 91, Entity Type: variable\n",
      " - Start: 101, End: 101, Entity Type: variable\n",
      " - Start: 109, End: 109, Entity Type: variable\n",
      " - Start: 119, End: 119, Entity Type: variable\n",
      " - Start: 132, End: 132, Entity Type: variable\n",
      " - Start: 142, End: 142, Entity Type: variable\n",
      " - Start: 150, End: 150, Entity Type: variable\n",
      " - Start: 160, End: 160, Entity Type: variable\n",
      " - Start: 77, End: 77, Entity Type: variable\n",
      " - Start: 95, End: 95, Entity Type: variable\n",
      " - Start: 113, End: 113, Entity Type: variable\n",
      " - Start: 123, End: 123, Entity Type: variable\n",
      " - Start: 169, End: 169, Entity Type: variable\n",
      " - Start: 180, End: 180, Entity Type: variable\n",
      " - Start: 215, End: 215, Entity Type: variable\n",
      " - Start: 223, End: 223, Entity Type: variable\n",
      " - Start: 79, End: 79, Entity Type: variable\n",
      " - Start: 136, End: 136, Entity Type: variable\n",
      " - Start: 154, End: 154, Entity Type: variable\n",
      " - Start: 164, End: 164, Entity Type: variable\n",
      " - Start: 192, End: 192, Entity Type: variable\n",
      " - Start: 203, End: 203, Entity Type: variable\n",
      " - Start: 217, End: 217, Entity Type: variable\n",
      " - Start: 81, End: 81, Entity Type: variable\n",
      " - Start: 173, End: 173, Entity Type: variable\n",
      " - Start: 184, End: 184, Entity Type: variable\n",
      " - Start: 221, End: 221, Entity Type: variable\n",
      " - Start: 83, End: 83, Entity Type: variable\n",
      " - Start: 196, End: 196, Entity Type: variable\n",
      " - Start: 207, End: 207, Entity Type: variable\n",
      " - Start: 226, End: 226, Entity Type: variable\n",
      " - Start: 85, End: 85, Entity Type: variable\n",
      " - Start: 219, End: 219, Entity Type: variable\n",
      " - Start: 224, End: 224, Entity Type: variable\n",
      " - Start: 236, End: 236, Entity Type: variable\n",
      "\n",
      "Negative Entities:\n",
      "['database', 'Date']\n"
     ]
    }
   ],
   "source": [
    "# Charger les données traitées pour inspection\n",
    "with open('pilener_train.json', 'r') as f:\n",
    "    processed_data = json.load(f)\n",
    "\n",
    "# Afficher un exemple\n",
    "example_idx = 0  # Modifier cet indice pour voir d'autres exemples\n",
    "example = processed_data[example_idx]\n",
    "\n",
    "# Afficher avec une mise en forme claire\n",
    "print(\"=== Exemple traité ===\")\n",
    "print(\"Tokenized Text:\")\n",
    "print(example['tokenized_text'])\n",
    "print(\"\\nNER Spans:\")\n",
    "for span in example['ner']:\n",
    "    print(f\" - Start: {span[0]}, End: {span[1]}, Entity Type: {span[2]}\")\n",
    "print(\"\\nNegative Entities:\")\n",
    "print(example['negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faut allez à lendroit ou est defini le model puis vous pouvez exec la suite dans lordre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GLiNER(\n",
       "  (encoder): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (entity_ffn): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=200, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "  )\n",
       "  (span_ffn): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=200, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (loss_fn): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GLiNER(pretrained_model_name=\"microsoft/deberta-v3-base\", span_max_length=2, hidden_size=200)\n",
    "# Initialiser le modèle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   0%|          | 0/45889 [00:00<?, ?entry/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing Data:  10%|▉         | 4385/45889 [00:14<02:05, 330.88entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  34%|███▍      | 15542/45889 [00:48<01:32, 327.34entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  38%|███▊      | 17456/45889 [00:54<01:25, 331.72entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  53%|█████▎    | 24186/45889 [01:15<01:06, 325.59entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  64%|██████▍   | 29371/45889 [01:31<00:50, 328.02entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  73%|███████▎  | 33381/45889 [01:44<00:45, 274.25entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  76%|███████▌  | 34910/45889 [01:49<00:34, 314.63entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  88%|████████▊ | 40333/45889 [02:06<00:16, 327.54entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, not same size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|██████████| 45889/45889 [02:24<00:00, 318.39entry/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def prepare_data_for_training(processed_data, model, max_length=64, max_entity_per_seq = 10):\n",
    "    input_ids, labels, entity_tensors, attention_masks = [], [], [], []\n",
    "    entity_masks, sentence_masks = [], []\n",
    "\n",
    "    # Créer un mapping des types d'entités vers des entiers\n",
    "    entity_types = {entity for entry in processed_data for _, _, entity in entry[\"ner\"]}\n",
    "    special_tokens = [f\"[ENT] {entity}\" for entity in entity_types]\n",
    "    model.tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "    model.encoder.resize_token_embeddings(len(model.tokenizer))\n",
    "\n",
    "    for entry in tqdm(processed_data, desc=\"Processing Data\", unit=\"entry\"):\n",
    "        tokenized_text = entry[\"tokenized_text\"]\n",
    "        ner_spans = entry[\"ner\"]\n",
    "                \n",
    "        # Générer le tensor de labels\n",
    "        label_tensor = torch.zeros(max_length, dtype=torch.long)\n",
    "        current_entity_id = []\n",
    "        current_entity_str = []\n",
    "\n",
    "        for start, end, entity_type in ner_spans:\n",
    "            if start < max_length and end < max_length and len(current_entity_str)<max_entity_per_seq:\n",
    "                entity_token_id = model.tokenizer.convert_tokens_to_ids(f'[ENT] {entity_type}')\n",
    "                label_tensor[start:end + 1] = entity_token_id\n",
    "\n",
    "                if entity_token_id not in current_entity_id:\n",
    "                    current_entity_id.append(entity_token_id)\n",
    "                if entity_type not in current_entity_str:\n",
    "                    current_entity_str.append(entity_type)\n",
    "\n",
    "        entity_tokens = \" \".join(f\"[ENT] {et}\" for et in current_entity_str)\n",
    "        \n",
    "        # Tokeniser la séquence principale\n",
    "        encoded = model.tokenizer(\n",
    "            tokenized_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, \n",
    "            is_split_into_words=True, add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        word_ids = encoded.word_ids()\n",
    "        first_subtoken_ids = [\n",
    "            encoded[\"input_ids\"][0, i].item() for i, word_id in enumerate(word_ids) \n",
    "            if word_id is not None and (i == 0 or word_ids[i - 1] != word_id)\n",
    "        ]\n",
    "\n",
    "        encoded_entity = model.tokenizer(\n",
    "            entity_tokens, return_tensors=\"pt\", padding=\"max_length\", truncation=True, \n",
    "            is_split_into_words=False, add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        if len(current_entity_str) != len(encoded_entity[\"input_ids\"][0]) or len(tokenized_text) != len(first_subtoken_ids):\n",
    "            print(\"error, not same size\")\n",
    "            continue\n",
    "\n",
    "        encoded_entity = encoded_entity[\"input_ids\"][0].tolist() + [0]*(max_entity_per_seq-len(current_entity_str))\n",
    "\n",
    "        sep_id = model.tokenizer.convert_tokens_to_ids(f'[SEP]')\n",
    "\n",
    "        combined_ids = (\n",
    "            encoded_entity +\n",
    "            [sep_id] +\n",
    "            first_subtoken_ids\n",
    "        )\n",
    "\n",
    "        if len(combined_ids) != max_entity_per_seq + len(first_subtoken_ids) + 1:\n",
    "            print(\"error, not same size\")\n",
    "            continue\n",
    "\n",
    "        deleted_ids = max(len(combined_ids) - max_length,0)\n",
    "        combined_ids = combined_ids[:max_length]\n",
    "        combined_ids += [0] * (max_length - len(combined_ids))\n",
    "\n",
    "        # Créer l'attention mask\n",
    "        attention_mask = [1 if id != 0 else 0 for id in combined_ids]\n",
    "\n",
    "        # Masques spécifiques pour les entités et la phrase\n",
    "        entity_mask = [1 if i < len(current_entity_str) else 0 for i in range(len(combined_ids))]\n",
    "        sentence_mask = [1 if i > len(encoded_entity) and combined_ids[i] != 0 and combined_ids[i] != sep_id else 0 \n",
    "                         for i in range(len(combined_ids))]\n",
    "\n",
    "        # Vérification des tailles\n",
    "        if sum(entity_mask) != len(current_entity_str):\n",
    "            print(f\"Entity mask size mismatch: {sum(entity_mask)} != {len(current_entity_str)}\")\n",
    "            continue\n",
    "        if sum(sentence_mask) != len(tokenized_text)-deleted_ids:\n",
    "            print(f\"Sentence mask size mismatch: {sum(sentence_mask)} != {len(tokenized_text)-deleted_ids}\")\n",
    "            continue\n",
    "\n",
    "        current_entity_id = current_entity_id + [0]*(max_entity_per_seq-len(current_entity_str))\n",
    "\n",
    "        # Convertir les entités en un tensor\n",
    "        entity_tensor = torch.tensor(current_entity_id, dtype=torch.long)\n",
    "        \n",
    "        # Ajouter les données\n",
    "        input_ids.append(torch.tensor(combined_ids, dtype=torch.long))\n",
    "        labels.append(label_tensor)\n",
    "        entity_tensors.append(entity_tensor)\n",
    "        attention_masks.append(torch.tensor(attention_mask, dtype=torch.long))\n",
    "        entity_masks.append(torch.tensor(entity_mask, dtype=torch.long))\n",
    "        sentence_masks.append(torch.tensor(sentence_mask, dtype=torch.long))\n",
    "\n",
    "    return (\n",
    "        torch.stack(input_ids), \n",
    "        torch.stack(labels), \n",
    "        entity_tensors, \n",
    "        torch.stack(attention_masks), \n",
    "        torch.stack(entity_masks), \n",
    "        torch.stack(sentence_masks)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Charger les données générées précédemment\n",
    "with open('pilener_train.json', 'r') as f:\n",
    "    processed_data = json.load(f)\n",
    "\n",
    "# Préparer les données avec suivi d'avancement\n",
    "input_ids, labels, entity_tensors, attention_masks, entity_masks, sentence_masks = prepare_data_for_training(processed_data, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143177\n",
      "45881\n",
      "45881\n"
     ]
    }
   ],
   "source": [
    "print(len(model.tokenizer))\n",
    "print(len(input_ids))\n",
    "print(len(entity_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de input_ids : torch.Size([45881, 64])\n",
      "Forme de attention_masks : torch.Size([45881, 64])\n",
      "Forme de labels : torch.Size([45881, 64])\n",
      "Forme de entity_masks : torch.Size([45881, 64])\n",
      "Forme de sentence_masks : torch.Size([45881, 64])\n",
      "\n",
      "Exemple de input_ids (première entrée) :\n",
      "tensor([138011, 136330, 141694,      0,      0,      0,      0,      0,      0,\n",
      "             0,      2,   1729,    877,  18172,   1470,    636,    277,   4648,\n",
      "         14321,    267,  96792,    273,    286,   1223,    266,   1571,    422,\n",
      "           319,    403,    930,    312,   1470,    636,    277,   4648,  14321,\n",
      "           304,    278,    382,   1550,    298,    801,    401,    278,    490,\n",
      "           298,    489,    930,    399,    273,    409,    278,    264,    263,\n",
      "          1733,    343,    269,    266,    493,    384,    265,    898,    278,\n",
      "           588])\n",
      "\n",
      "Exemple de attention_masks (première entrée) :\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "Exemple de labels (première entrée) :\n",
      "tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "        136330,      0,      0,      0,      0, 138011,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0, 141694,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0])\n",
      "\n",
      "Exemple de entity_tensors (première entrée) :\n",
      "tensor([138011, 136330, 141694,      0,      0,      0,      0,      0,      0,\n",
      "             0])\n",
      "\n",
      "Exemple de entity_masks (première entrée) :\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "Exemple de sentence_masks (première entrée) :\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Afficher les formes des tensors principaux\n",
    "print(f\"Forme de input_ids : {input_ids.shape}\")\n",
    "print(f\"Forme de attention_masks : {attention_masks.shape}\")\n",
    "print(f\"Forme de labels : {labels.shape}\")\n",
    "print(f\"Forme de entity_masks : {entity_masks.shape}\")\n",
    "print(f\"Forme de sentence_masks : {sentence_masks.shape}\")\n",
    "\n",
    "# Afficher un exemple pour les tensors principaux\n",
    "print(\"\\nExemple de input_ids (première entrée) :\")\n",
    "print(input_ids[0])\n",
    "\n",
    "print(\"\\nExemple de attention_masks (première entrée) :\")\n",
    "print(attention_masks[0])\n",
    "\n",
    "print(\"\\nExemple de labels (première entrée) :\")\n",
    "print(labels[0])\n",
    "\n",
    "# Afficher un exemple pour les entity_tensors\n",
    "print(\"\\nExemple de entity_tensors (première entrée) :\")\n",
    "print(entity_tensors[0])\n",
    "\n",
    "# Afficher un exemple pour les entity_masks\n",
    "print(\"\\nExemple de entity_masks (première entrée) :\")\n",
    "print(entity_masks[0])\n",
    "\n",
    "# Afficher un exemple pour les sentence_masks\n",
    "print(\"\\nExemple de sentence_masks (première entrée) :\")\n",
    "print(sentence_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels, entities,entity_masks,sentence_masks , max_span_length=2):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.entity_masks = entity_masks\n",
    "        self.sentence_masks =sentence_masks\n",
    "        self.labels = labels  # Liste des labels pour chaque token\n",
    "        self.entities = entities  # Liste des entités uniques\n",
    "        self.max_span_length = max_span_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_id = self.input_ids[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        token_labels = self.labels[idx]  # Labels token-par-token\n",
    "        entity_ids = self.entities[idx]  # Entités pour cet exemple\n",
    "\n",
    "        num_tokens = len(input_id) - len(entity_ids) - 1\n",
    "        spans = [\n",
    "            (start, end)\n",
    "            for start in range(num_tokens)\n",
    "            for end in range(start, min(start + self.max_span_length, num_tokens))\n",
    "        ]\n",
    "        num_spans = len(spans)\n",
    "        num_entities = len(entity_ids)\n",
    "\n",
    "        # Matrice binaire : spans x entities\n",
    "        binary_labels = torch.zeros(num_spans, num_entities, dtype=torch.float)\n",
    "\n",
    "        for span_idx, (start, end) in enumerate(spans):\n",
    "            span_labels = token_labels[start:end + 1]\n",
    "            for entity_idx, entity_id in enumerate(entity_ids):\n",
    "                if all((label == entity_id and label != 0)  for label in span_labels):\n",
    "                    binary_labels[span_idx, entity_idx] = 1\n",
    "\n",
    "        return input_id, attention_mask, spans, entity_ids, binary_labels, sentence_masks, entity_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = int(len(input_ids) * proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID: tensor([138011, 136330, 141694,      0,      0,      0,      0,      0,      0,\n",
      "             0,      2,   1729,    877,  18172,   1470,    636,    277,   4648,\n",
      "         14321,    267,  96792,    273,    286,   1223,    266,   1571,    422,\n",
      "           319,    403,    930,    312,   1470,    636,    277,   4648,  14321,\n",
      "           304,    278,    382,   1550,    298,    801,    401,    278,    490,\n",
      "           298,    489,    930,    399,    273,    409,    278,    264,    263,\n",
      "          1733,    343,    269,    266,    493,    384,    265,    898,    278,\n",
      "           588])\n",
      "Spans: [(0, 0), (0, 1), (1, 1), (1, 2), (2, 2), (2, 3), (3, 3), (3, 4), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (6, 7), (7, 7), (7, 8), (8, 8), (8, 9), (9, 9), (9, 10), (10, 10), (10, 11), (11, 11), (11, 12), (12, 12), (12, 13), (13, 13), (13, 14), (14, 14), (14, 15), (15, 15), (15, 16), (16, 16), (16, 17), (17, 17), (17, 18), (18, 18), (18, 19), (19, 19), (19, 20), (20, 20), (20, 21), (21, 21), (21, 22), (22, 22), (22, 23), (23, 23), (23, 24), (24, 24), (24, 25), (25, 25), (25, 26), (26, 26), (26, 27), (27, 27), (27, 28), (28, 28), (28, 29), (29, 29), (29, 30), (30, 30), (30, 31), (31, 31), (31, 32), (32, 32), (32, 33), (33, 33), (33, 34), (34, 34), (34, 35), (35, 35), (35, 36), (36, 36), (36, 37), (37, 37), (37, 38), (38, 38), (38, 39), (39, 39), (39, 40), (40, 40), (40, 41), (41, 41), (41, 42), (42, 42), (42, 43), (43, 43), (43, 44), (44, 44), (44, 45), (45, 45), (45, 46), (46, 46), (46, 47), (47, 47), (47, 48), (48, 48), (48, 49), (49, 49), (49, 50), (50, 50), (50, 51), (51, 51), (51, 52), (52, 52)]\n",
      "Entity IDs: tensor([138011, 136330, 141694,      0,      0,      0,      0,      0,      0,\n",
      "             0])\n",
      "Binary Labels: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Créer le dataset\n",
    "dataset = NERDataset(input_ids[:ind], attention_masks[:ind], labels[:ind], entity_tensors[:ind],entity_masks[:ind],sentence_masks[:ind])\n",
    "\n",
    "# Exemple de récupération d'une entrée\n",
    "input_id, attention_mask, spans, entity_ids, binary_labels, sentence_masks, entity_masks = dataset[0]\n",
    "\n",
    "print(\"Input ID:\", input_id)\n",
    "print(\"Spans:\", spans)\n",
    "print(\"Entity IDs:\", entity_ids)\n",
    "print(\"Binary Labels:\", binary_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks = zip(*batch)\n",
    "\n",
    "    # Trouver les tailles maximales pour le padding\n",
    "    max_len = max(len(ids) for ids in input_ids)  # Longueur max des tokens\n",
    "    max_spans = max(len(s) for s in spans)  # Nombre max de spans\n",
    "    max_entities = max(len(e) for e in entity_ids)  # Nombre max d'entités\n",
    "\n",
    "    # Padding des input_ids et attention_masks\n",
    "    # padded_input_ids = torch.stack([\n",
    "    #     torch.cat([ids, torch.zeros(max_len - len(ids), dtype=torch.long)])\n",
    "    #     for ids in input_ids\n",
    "    # ])\n",
    "    # padded_attention_masks = torch.stack([\n",
    "    #     torch.cat([mask, torch.zeros(max_len - len(mask), dtype=torch.long)])\n",
    "    #     for mask in attention_masks\n",
    "    # ])\n",
    "\n",
    "    # Padding des spans\n",
    "    # spans = torch.stack([\n",
    "    #     torch.cat([torch.tensor(s, dtype=torch.long), torch.zeros((max_spans - len(s), 2), dtype=torch.long)])\n",
    "    #     for s in spans\n",
    "    # ])\n",
    "\n",
    "    # Padding des entity_ids\n",
    "    # padded_entity_ids = torch.stack([\n",
    "    #     torch.cat([e, torch.zeros(max_entities - len(e), dtype=torch.long)])\n",
    "    #     for e in entity_ids\n",
    "    # ])\n",
    "\n",
    "    # Padding des binary_labels\n",
    "    # binary_labels = torch.stack([\n",
    "    #     torch.cat([\n",
    "    #         torch.cat([bl, torch.zeros(max_spans - bl.size(0), bl.size(1))], dim=0) if bl.size(0) < max_spans else bl,\n",
    "    #         torch.zeros(max_spans, max_entities - bl.size(1)) if bl.size(1) < max_entities else torch.zeros(0)\n",
    "    #     ], dim=1)\n",
    "    #     for bl in binary_labels\n",
    "    # ])\n",
    "\n",
    "        # Conversion en tensors\n",
    "        #spans = [torch.tensor(s, dtype=torch.long) for s in spans]\n",
    "    input_ids = torch.stack([ids.clone().detach() for ids in input_ids])\n",
    "    attention_masks = torch.stack([mask.clone().detach() for mask in attention_masks]) \n",
    "    entity_ids = torch.stack([e.clone().detach() for e in entity_ids])\n",
    "    binary_labels = torch.stack([bl.clone().detach() for bl in binary_labels])\n",
    "    sentence_masks = torch.stack([sm.clone().detach() for sm in sentence_masks])\n",
    "    entity_masks = torch.stack([em.clone().detach() for em in entity_masks])\n",
    "    spans = torch.tensor([span for span in spans], dtype=torch.long)\n",
    "\n",
    "    return input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4588\n",
      "Validation size: 574\n",
      "Test size: 574\n"
     ]
    }
   ],
   "source": [
    "# Définir les proportions pour le train, validation et test\n",
    "train_ratio = 0.8  # 80% des données pour l'entraînement\n",
    "val_ratio = 0.1    # 10% des données pour la validation\n",
    "test_ratio = 0.1   # 10% des données pour le test\n",
    "\n",
    "# Calculer les tailles des différents ensembles\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = int(val_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Diviser les données en train, validation, et test\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Créer les DataLoaders pour chaque ensemble\n",
    "batch_size = 8  # Ajuster selon vos besoins\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Vérification des tailles\n",
    "print(f\"Train size: {len(train_loader)}\")\n",
    "print(f\"Validation size: {len(val_loader)}\")\n",
    "print(f\"Test size: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class GLiNER(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"microsoft/deberta-v3-base\", span_max_length=2, hidden_size=768, dropout_rate=0.4):\n",
    "        super(GLiNER, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        self.encoder_output_size = self.encoder.config.hidden_size\n",
    "        self.entity_ffn = nn.Sequential(\n",
    "            nn.Linear(self.encoder_output_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.span_ffn = nn.Sequential(\n",
    "            nn.Linear(2 * self.encoder_output_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.span_max_length = span_max_length\n",
    "        self.loss_fn = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "    def forward(self, input_ids, attention_masks, entity_types, spans, sentence_masks, entity_masks, binary_labels=None):\n",
    "        # print(\"Input IDs shape:\", input_ids.shape)\n",
    "        # print(\"Attention mask shape:\", attention_masks.shape)\n",
    "        # Passer input_ids et attention_masks au modèle\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "    \n",
    "        entity_embeddings, text_embeddings = self.split_embeddings(token_embeddings,len(entity_types[0]))\n",
    "        \n",
    "\n",
    "        refined_entity_embeddings = self.entity_ffn(entity_embeddings)\n",
    "        \n",
    "        span_scores = self.compute_span_scores(refined_entity_embeddings, text_embeddings, spans)\n",
    "\n",
    "        if binary_labels is not None:\n",
    "            loss = self.compute_loss(span_scores, binary_labels)\n",
    "            return span_scores, loss\n",
    "        \n",
    "        return span_scores\n",
    "\n",
    "\n",
    "    def split_embeddings(self, token_embeddings, num_entity_types = 25):\n",
    "        entity_embeddings = token_embeddings[:, 0:num_entity_types, :]\n",
    "        text_embeddings = token_embeddings[:, num_entity_types + 1:, :]\n",
    "        \n",
    "        return entity_embeddings, text_embeddings\n",
    "\n",
    "    \n",
    "    def compute_span_scores(self, entity_embeddings, text_embeddings, spans):\n",
    "        \"\"\"\n",
    "        Calcule les scores des spans en une seule passe vectorisée, \n",
    "        en supposant que tous les spans sont valides.\n",
    "        \"\"\"\n",
    "        batch_size, text_length, hidden_size = text_embeddings.shape\n",
    "\n",
    "        # Conversion des spans en tensor directement\n",
    "        spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
    "\n",
    "        # Récupération des embeddings des spans\n",
    "        i_indices = spans_tensor[:, :, 0].unsqueeze(-1).expand(-1, -1, hidden_size)  # (batch, num_spans, hidden_size)\n",
    "        j_indices = spans_tensor[:, :, 1].unsqueeze(-1).expand(-1, -1, hidden_size)\n",
    "\n",
    "        start_embeddings = torch.gather(text_embeddings, 1, i_indices)  # (batch, num_spans, hidden_size)\n",
    "        end_embeddings = torch.gather(text_embeddings, 1, j_indices)    # (batch, num_spans, hidden_size)\n",
    "\n",
    "        # Concaténer les embeddings des extrémités et passer dans la FFN\n",
    "        span_reprs = torch.cat([start_embeddings, end_embeddings], dim=-1)  # (batch, num_spans, 2 * hidden_size)\n",
    "        span_reprs = self.span_ffn(span_reprs)                              # (batch, num_spans, hidden_size)\n",
    "\n",
    "        # Calcul des scores pour toutes les entités\n",
    "        scores = torch.einsum(\"bsh,beh->bse\", span_reprs, entity_embeddings)  # (batch, num_spans, num_entity_types)\n",
    "\n",
    "        # Appliquer la sigmoïde pour les scores finaux\n",
    "        span_scores = self.sigmoid(scores)\n",
    "\n",
    "        return span_scores\n",
    "\n",
    "\n",
    "    def compute_loss(self, span_scores, binary_labels):\n",
    "        \"\"\"\n",
    "        Calcul de la perte binaire cross-entropy entre les scores et les étiquettes.\n",
    "        \"\"\"\n",
    "        # print(f\"span_scores shape: {span_scores.shape}\")\n",
    "        # print(f\"binary_labels shape: {binary_labels.shape}\")\n",
    "\n",
    "        # Appliquer la perte\n",
    "        loss = self.loss_fn(span_scores, binary_labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/4588 [00:00<?, ?batch/s]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_11204\\1185905307.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Training:   3%|▎         | 158/4588 [01:10<33:02,  2.23batch/s, Batch Loss=0.191]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m     global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "Cell \u001b[1;32mIn[14], line 41\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     39\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m---> 41\u001b[0m     input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks \u001b[38;5;241m=\u001b[39m [\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     45\u001b[0m     span_scores, loss \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m     46\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m     47\u001b[0m         attention_masks\u001b[38;5;241m=\u001b[39mattention_masks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m         entity_masks\u001b[38;5;241m=\u001b[39mentity_masks\n\u001b[0;32m     53\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparamètres\n",
    "num_epochs = 5\n",
    "# learning_rate = 1e-5\n",
    "\n",
    "# # Optimiseur et scheduler\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "# scheduler = get_scheduler(\n",
    "#     \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * num_epochs\n",
    "# )\n",
    "\n",
    "# Calcul des étapes totales\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "# Optimiseur avec deux taux d'apprentissage\n",
    "optimizer = AdamW([\n",
    "    {'params': model.encoder.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.entity_ffn.parameters(), 'lr': 5e-5},\n",
    "    {'params': model.span_ffn.parameters(), 'lr': 5e-5},\n",
    "])\n",
    "\n",
    "# Scheduler cosinus\n",
    "scheduler = get_scheduler(\n",
    "    \"cosine\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "# Fonction d'entraînement avec tqdm\n",
    "def train_epoch(model, train_loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Ajout de tqdm pour afficher la progression\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        span_scores, loss = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_masks=attention_masks,\n",
    "            entity_types=entity_ids,\n",
    "            spans=spans,\n",
    "            binary_labels=binary_labels,\n",
    "            sentence_masks=sentence_masks,\n",
    "            entity_masks=entity_masks\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Mise à jour de la barre de progression\n",
    "        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# # Boucle d'entraînement avec tqdm\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "#     print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "max_steps = 30000\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    global_step += len(train_loader)\n",
    "    if global_step >= max_steps:\n",
    "        print(\"Reached maximum training steps.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Fonction pour calculer la précision, le rappel, le F1-score et la matrice de confusion\n",
    "def calculate_metrics(binary_scores, binary_labels):\n",
    "    # Déplacer les tensors vers le CPU avant de les convertir en NumPy\n",
    "    binary_scores_flat = binary_scores.cpu().flatten()\n",
    "    binary_labels_flat = binary_labels.cpu().flatten()\n",
    "\n",
    "    # Calcul des métriques\n",
    "    precision = precision_score(binary_labels_flat, binary_scores_flat)\n",
    "    recall = recall_score(binary_labels_flat, binary_scores_flat)\n",
    "    f1 = f1_score(binary_labels_flat, binary_scores_flat)\n",
    "\n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(binary_labels_flat, binary_scores_flat)\n",
    "\n",
    "    return precision, recall, f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, normalize=False):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    # Normalisation de la matrice de confusion si nécessaire\n",
    "    if normalize:\n",
    "        confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    print(confusion_matrix)\n",
    "    # Affichage de la matrice de confusion avec format ajusté\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='.4f' if normalize else 'g', cmap='Blues', cbar=False, \n",
    "                xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "                yticklabels=['True Negative', 'True Positive'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, test_loader):\n",
    "    model.eval()  # Passer en mode évaluation\n",
    "    total_loss = 0\n",
    "    test_loss = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    confusion_matrix_total = torch.zeros(2, 2)  # Confusion matrix for binary classification\n",
    "\n",
    "    progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n",
    "    with torch.no_grad():  # Désactiver les gradients pour la phase de test\n",
    "        for batch in progress_bar:\n",
    "            input_ids, attention_masks, spans, entity_ids, binary_labels, sentence_masks, entity_masks = [b.to(device) for b in batch]\n",
    "\n",
    "            span_scores, loss = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_masks=attention_masks,\n",
    "                entity_types=entity_ids,\n",
    "                spans=spans,\n",
    "                binary_labels=binary_labels,\n",
    "                sentence_masks=sentence_masks,\n",
    "                entity_masks=entity_masks\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calcul des autres métriques\n",
    "            binary_scores = (span_scores >= 0.5).float()  # Convertir les scores en prédictions binaires\n",
    "            precision, recall, f1, cm = calculate_metrics(binary_scores, binary_labels)\n",
    "            \n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "            confusion_matrix_total += torch.tensor(cm)\n",
    "\n",
    "            # Mise à jour de la barre de progression\n",
    "            progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    # Moyenne des métriques\n",
    "    average_precision = total_precision / len(test_loader)\n",
    "    average_recall = total_recall / len(test_loader)\n",
    "    average_f1 = total_f1 / len(test_loader)\n",
    "    confusion_matrix_total = confusion_matrix_total.numpy()\n",
    "\n",
    "    return avg_loss, average_precision,average_recall,average_f1,confusion_matrix_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/29 [00:00<?, ?batch/s]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:   3%|▎         | 1/29 [00:01<00:32,  1.17s/batch, Batch Loss=0.0199]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:   7%|▋         | 2/29 [00:02<00:30,  1.13s/batch, Batch Loss=0.0169]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  10%|█         | 3/29 [00:03<00:29,  1.12s/batch, Batch Loss=0.0145]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  14%|█▍        | 4/29 [00:04<00:29,  1.17s/batch, Batch Loss=0.0146]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  17%|█▋        | 5/29 [00:05<00:27,  1.16s/batch, Batch Loss=0.0175]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  21%|██        | 6/29 [00:06<00:26,  1.16s/batch, Batch Loss=0.00921]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  24%|██▍       | 7/29 [00:08<00:25,  1.15s/batch, Batch Loss=0.0157] C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  28%|██▊       | 8/29 [00:09<00:24,  1.16s/batch, Batch Loss=0.0113]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  31%|███       | 9/29 [00:10<00:23,  1.20s/batch, Batch Loss=0.014] C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  34%|███▍      | 10/29 [00:11<00:22,  1.18s/batch, Batch Loss=0.0113]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  38%|███▊      | 11/29 [00:12<00:21,  1.17s/batch, Batch Loss=0.0136]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  41%|████▏     | 12/29 [00:14<00:20,  1.19s/batch, Batch Loss=0.0269]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  45%|████▍     | 13/29 [00:15<00:19,  1.20s/batch, Batch Loss=0.0122]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  48%|████▊     | 14/29 [00:16<00:17,  1.20s/batch, Batch Loss=0.0137]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  52%|█████▏    | 15/29 [00:17<00:16,  1.17s/batch, Batch Loss=0.0216]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  55%|█████▌    | 16/29 [00:18<00:15,  1.17s/batch, Batch Loss=0.013] C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  59%|█████▊    | 17/29 [00:19<00:13,  1.16s/batch, Batch Loss=0.0218]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  62%|██████▏   | 18/29 [00:21<00:12,  1.17s/batch, Batch Loss=0.0153]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  66%|██████▌   | 19/29 [00:22<00:11,  1.17s/batch, Batch Loss=0.0213]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  69%|██████▉   | 20/29 [00:23<00:10,  1.15s/batch, Batch Loss=0.0212]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  72%|███████▏  | 21/29 [00:24<00:09,  1.15s/batch, Batch Loss=0.0158]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  76%|███████▌  | 22/29 [00:25<00:07,  1.14s/batch, Batch Loss=0.028] C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  79%|███████▉  | 23/29 [00:26<00:06,  1.13s/batch, Batch Loss=0.0166]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "Testing:  83%|████████▎ | 24/29 [00:27<00:05,  1.14s/batch, Batch Loss=0.0186]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  86%|████████▌ | 25/29 [00:29<00:04,  1.14s/batch, Batch Loss=0.00934]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  90%|████████▉ | 26/29 [00:30<00:03,  1.14s/batch, Batch Loss=0.0183] C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  93%|█████████▎| 27/29 [00:31<00:02,  1.14s/batch, Batch Loss=0.0175]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing:  97%|█████████▋| 28/29 [00:32<00:01,  1.15s/batch, Batch Loss=0.0192]C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n",
      "c:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Testing: 100%|██████████| 29/29 [00:33<00:00,  1.15s/batch, Batch Loss=0.0144]\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_precision, test_recall, test_f1, cm = test_epoch(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0167\n",
      "Test Precision: 0.2532\n",
      "Test Recall: 0.0133\n",
      "Test F1 Score: 0.0240\n",
      "[[9.99967275e-01 3.27254720e-05]\n",
      " [9.87160494e-01 1.28395062e-02]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "plot_confusion_matrix(cm,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cm[0, 0] = TN\n",
    "- cm[0, 1] = FP\n",
    "- cm[1, 0] = FN\n",
    "- cm[1, 1] = TP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_fulltiny.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#load \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_fulltiny.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\torch\\serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\torch\\serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\vincentorch\\Lib\\site-packages\\torch\\serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_fulltiny.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#load \n",
    "model = torch.load(\"model_fulltiny.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle complet\n",
    "torch.save(model, \"model_fulltiny.pth\")\n",
    "# Sauvegarder le modèle\n",
    "torch.save(model.state_dict(), \"modeltiny.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase d'exemple\n",
    "sentence = \"Alain Farley works at McGill University\"\n",
    "entity_types_to_detect = [\"person\", \"organization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"Marie Dupont is a data scientist at OpenAI and she specializes in Python programming.\"\n",
    "# entity_types_to_detect = [\"person\", \"organization\",\"programming language\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_entity_per_seq = 25\n",
    "max_length = 128\n",
    "max_span_length = 2\n",
    "threshold_score = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_entity_id = []\n",
    "current_entity_str = []\n",
    "\n",
    "for entity_type in entity_types_to_detect:\n",
    "    entity_token_id = model.tokenizer.convert_tokens_to_ids(f'[ENT] {entity_type}')\n",
    "    if entity_token_id not in current_entity_id:\n",
    "        current_entity_id.append(entity_token_id)\n",
    "    if entity_type not in current_entity_str:\n",
    "        current_entity_str.append(entity_type)\n",
    "\n",
    "entity_tokens = \" \".join(f\"[ENT] {et}\" for et in current_entity_str)\n",
    "\n",
    "# Tokeniser la séquence principale\n",
    "encoded = model.tokenizer(\n",
    "    sentence.split(), return_tensors=\"pt\", padding=\"max_length\", truncation=True, \n",
    "    is_split_into_words=True, add_special_tokens=False\n",
    ")\n",
    "\n",
    "word_ids = encoded.word_ids()\n",
    "first_subtoken_ids = [\n",
    "    encoded[\"input_ids\"][0, i].item() for i, word_id in enumerate(word_ids) \n",
    "    if word_id is not None and (i == 0 or word_ids[i - 1] != word_id)\n",
    "]\n",
    "\n",
    "encoded_entity = model.tokenizer(\n",
    "    entity_tokens, return_tensors=\"pt\", padding=\"max_length\", truncation=True, \n",
    "    is_split_into_words=False, add_special_tokens=False\n",
    ")\n",
    "\n",
    "encoded_entity = encoded_entity[\"input_ids\"][0].tolist() + [0]*(max_entity_per_seq-len(current_entity_str))\n",
    "\n",
    "sep_id = model.tokenizer.convert_tokens_to_ids(f'[SEP]')\n",
    "\n",
    "combined_ids = (\n",
    "    encoded_entity +\n",
    "    [sep_id] +\n",
    "    first_subtoken_ids\n",
    ")\n",
    "\n",
    "deleted_ids = max(len(combined_ids) - max_length,0)\n",
    "combined_ids = combined_ids[:max_length]\n",
    "combined_ids += [0] * (max_length - len(combined_ids))\n",
    "\n",
    "# Créer l'attention mask\n",
    "attention_mask = [1 if id != 0 else 0 for id in combined_ids]\n",
    "\n",
    "# Masques spécifiques pour les entités et la phrase\n",
    "entity_mask = [1 if i < len(current_entity_str) else 0 for i in range(len(combined_ids))]\n",
    "sentence_mask = [1 if i > len(encoded_entity) and combined_ids[i] != 0 and combined_ids[i] != sep_id else 0 \n",
    "                    for i in range(len(combined_ids))]\n",
    "\n",
    "current_entity_id = current_entity_id + [0]*(max_entity_per_seq-len(current_entity_str))\n",
    "\n",
    "# Convertir les entités en un tensor\n",
    "entity_tensor = torch.tensor(current_entity_id, dtype=torch.long)\n",
    "\n",
    "# Ajouter les données\n",
    "input_ids_tensor = torch.tensor(combined_ids, dtype=torch.long)\n",
    "attention_mask_tensor = torch.tensor(attention_mask, dtype=torch.long)\n",
    "entity_mask_tensor =torch.tensor(entity_mask, dtype=torch.long)\n",
    "sentence_mask_tensor = torch.tensor(sentence_mask, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(input_ids_tensor) - len(entity_tensor) - 1\n",
    "spans = [\n",
    "    (start, end)\n",
    "    for start in range(num_tokens)\n",
    "    for end in range(start, min(start + max_span_length, num_tokens))\n",
    "]\n",
    "\n",
    "spans_tensor = torch.tensor(spans, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doria\\AppData\\Local\\Temp\\ipykernel_35248\\1646548156.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spans_tensor = torch.stack([torch.tensor(s, device=text_embeddings.device) for s in spans])  # (batch, num_spans, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span scores: tensor([[[1.2001e-01, 2.3965e-01, 4.2827e-09,  ..., 4.2827e-09,\n",
      "          4.2827e-09, 4.2827e-09],\n",
      "         [1.3560e-01, 2.5948e-01, 9.1415e-09,  ..., 9.1415e-09,\n",
      "          9.1415e-09, 9.1415e-09],\n",
      "         [1.4347e-01, 2.5460e-01, 2.3091e-08,  ..., 2.3091e-08,\n",
      "          2.3091e-08, 2.3091e-08],\n",
      "         ...,\n",
      "         [2.5727e-04, 2.6614e-04, 3.6147e-09,  ..., 3.6147e-09,\n",
      "          3.6147e-09, 3.6147e-09],\n",
      "         [2.5727e-04, 2.6614e-04, 3.6147e-09,  ..., 3.6147e-09,\n",
      "          3.6147e-09, 3.6147e-09],\n",
      "         [2.5727e-04, 2.6614e-04, 3.6147e-09,  ..., 3.6147e-09,\n",
      "          3.6147e-09, 3.6147e-09]]], device='cuda:0')\n",
      "Binary Span Scores: tensor([[[1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0', dtype=torch.int32)\n",
      "Nombre de 1: 12\n",
      "Masked Binary Span Scores: tensor([[[0, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0', dtype=torch.int32)\n",
      "Masked Binary Span Scores: tensor([[[0, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Passage en mode évaluation\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "# Préparation des données de test\n",
    "# Assurez-vous que `input_ids_tensor`, `attention_mask_tensor`, `entity_tensor`, \n",
    "# `entity_mask_tensor`, `sentence_mask_tensor`, et `spans` soient bien définis.\n",
    "\n",
    "with torch.no_grad():  # Pas besoin de calculer les gradients en mode test\n",
    "    span_scores = model(\n",
    "        input_ids=input_ids_tensor.unsqueeze(0).to(device),  # Ajout d'une dimension batch\n",
    "        attention_masks=attention_mask_tensor.unsqueeze(0).to(device),\n",
    "        entity_types=entity_tensor.unsqueeze(0).to(device),\n",
    "        spans=spans_tensor.unsqueeze(0).to(device),\n",
    "        sentence_masks=sentence_mask_tensor.unsqueeze(0).to(device),\n",
    "        entity_masks=entity_mask_tensor.unsqueeze(0).to(device)\n",
    "    )\n",
    "\n",
    "# span_scores contient les scores prédites pour chaque span et chaque entité\n",
    "print(\"Span scores:\", span_scores)\n",
    "\n",
    "# Conversion des span scores en valeurs binaires\n",
    "binary_span_scores = (span_scores > threshold_score).int()\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Binary Span Scores:\", binary_span_scores)\n",
    "\n",
    "# Comptage des valeurs 1 dans tout le tenseur\n",
    "num_ones = binary_span_scores.sum().item()\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Nombre de 1:\", num_ones)\n",
    "\n",
    "# Création d'un masque avec des 0 et 1 représentant la valeur la plus élevée dans chaque liste\n",
    "# Initialisation d'un masque de mêmes dimensions que span_scores\n",
    "max_mask = torch.zeros_like(span_scores, dtype=torch.int)\n",
    "\n",
    "# Parcourir chaque batch, span et entité pour identifier les indices des max\n",
    "for i in range(span_scores.size(1)):  # Dimension des spans\n",
    "    for j in range(span_scores.size(2)):  # Dimension des entités\n",
    "        # Trouver l'indice de la valeur maximale dans la liste\n",
    "        max_index = torch.argmax(span_scores[0, i, :])  # Corrigé pour les dimensions\n",
    "        # Définir 1 à cet indice dans le masque\n",
    "        max_mask[0, i, max_index] = 1  # Corrigé pour les dimensions\n",
    "\n",
    "# Appliquer le masque sur binary_span_scores\n",
    "masked_binary_span_scores = binary_span_scores * max_mask\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Masked Binary Span Scores:\", masked_binary_span_scores)\n",
    "\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Masked Binary Span Scores:\", masked_binary_span_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Span Scores with Spans:\n",
      "Example 1:\n",
      "      Span  person  organization\n",
      "    (0, 0)       0             1\n",
      "    (0, 1)       0             1\n",
      "    (1, 1)       0             1\n",
      "    (1, 2)       0             0\n",
      "    (2, 2)       0             0\n",
      "    (2, 3)       0             0\n",
      "    (3, 3)       0             0\n",
      "    (3, 4)       0             0\n",
      "    (4, 4)       0             1\n",
      "    (4, 5)       0             1\n",
      "    (5, 5)       0             1\n",
      "    (5, 6)       0             0\n",
      "    (6, 6)       0             0\n",
      "    (6, 7)       0             0\n",
      "    (7, 7)       0             0\n",
      "    (7, 8)       0             0\n",
      "    (8, 8)       0             0\n",
      "    (8, 9)       0             0\n",
      "    (9, 9)       0             0\n",
      "   (9, 10)       0             0\n",
      "  (10, 10)       0             0\n",
      "  (10, 11)       0             0\n",
      "  (11, 11)       0             0\n",
      "  (11, 12)       0             0\n",
      "  (12, 12)       0             0\n",
      "  (12, 13)       0             0\n",
      "  (13, 13)       0             0\n",
      "  (13, 14)       0             0\n",
      "  (14, 14)       0             0\n",
      "  (14, 15)       0             0\n",
      "  (15, 15)       0             0\n",
      "  (15, 16)       0             0\n",
      "  (16, 16)       0             0\n",
      "  (16, 17)       0             0\n",
      "  (17, 17)       0             0\n",
      "  (17, 18)       0             0\n",
      "  (18, 18)       0             0\n",
      "  (18, 19)       0             0\n",
      "  (19, 19)       0             0\n",
      "  (19, 20)       0             0\n",
      "  (20, 20)       0             0\n",
      "  (20, 21)       0             0\n",
      "  (21, 21)       0             0\n",
      "  (21, 22)       0             0\n",
      "  (22, 22)       0             0\n",
      "  (22, 23)       0             0\n",
      "  (23, 23)       0             0\n",
      "  (23, 24)       0             0\n",
      "  (24, 24)       0             0\n",
      "  (24, 25)       0             0\n",
      "  (25, 25)       0             0\n",
      "  (25, 26)       0             0\n",
      "  (26, 26)       0             0\n",
      "  (26, 27)       0             0\n",
      "  (27, 27)       0             0\n",
      "  (27, 28)       0             0\n",
      "  (28, 28)       0             0\n",
      "  (28, 29)       0             0\n",
      "  (29, 29)       0             0\n",
      "  (29, 30)       0             0\n",
      "  (30, 30)       0             0\n",
      "  (30, 31)       0             0\n",
      "  (31, 31)       0             0\n",
      "  (31, 32)       0             0\n",
      "  (32, 32)       0             0\n",
      "  (32, 33)       0             0\n",
      "  (33, 33)       0             0\n",
      "  (33, 34)       0             0\n",
      "  (34, 34)       0             0\n",
      "  (34, 35)       0             0\n",
      "  (35, 35)       0             0\n",
      "  (35, 36)       0             0\n",
      "  (36, 36)       0             0\n",
      "  (36, 37)       0             0\n",
      "  (37, 37)       0             0\n",
      "  (37, 38)       0             0\n",
      "  (38, 38)       0             0\n",
      "  (38, 39)       0             0\n",
      "  (39, 39)       0             0\n",
      "  (39, 40)       0             0\n",
      "  (40, 40)       0             0\n",
      "  (40, 41)       0             0\n",
      "  (41, 41)       0             0\n",
      "  (41, 42)       0             0\n",
      "  (42, 42)       0             0\n",
      "  (42, 43)       0             0\n",
      "  (43, 43)       0             0\n",
      "  (43, 44)       0             0\n",
      "  (44, 44)       0             0\n",
      "  (44, 45)       0             0\n",
      "  (45, 45)       0             0\n",
      "  (45, 46)       0             0\n",
      "  (46, 46)       0             0\n",
      "  (46, 47)       0             0\n",
      "  (47, 47)       0             0\n",
      "  (47, 48)       0             0\n",
      "  (48, 48)       0             0\n",
      "  (48, 49)       0             0\n",
      "  (49, 49)       0             0\n",
      "  (49, 50)       0             0\n",
      "  (50, 50)       0             0\n",
      "  (50, 51)       0             0\n",
      "  (51, 51)       0             0\n",
      "  (51, 52)       0             0\n",
      "  (52, 52)       0             0\n",
      "  (52, 53)       0             0\n",
      "  (53, 53)       0             0\n",
      "  (53, 54)       0             0\n",
      "  (54, 54)       0             0\n",
      "  (54, 55)       0             0\n",
      "  (55, 55)       0             0\n",
      "  (55, 56)       0             0\n",
      "  (56, 56)       0             0\n",
      "  (56, 57)       0             0\n",
      "  (57, 57)       0             0\n",
      "  (57, 58)       0             0\n",
      "  (58, 58)       0             0\n",
      "  (58, 59)       0             0\n",
      "  (59, 59)       0             0\n",
      "  (59, 60)       0             0\n",
      "  (60, 60)       0             0\n",
      "  (60, 61)       0             0\n",
      "  (61, 61)       0             0\n",
      "  (61, 62)       0             0\n",
      "  (62, 62)       0             0\n",
      "  (62, 63)       0             0\n",
      "  (63, 63)       0             0\n",
      "  (63, 64)       0             0\n",
      "  (64, 64)       0             0\n",
      "  (64, 65)       0             0\n",
      "  (65, 65)       0             0\n",
      "  (65, 66)       0             0\n",
      "  (66, 66)       0             0\n",
      "  (66, 67)       0             0\n",
      "  (67, 67)       0             0\n",
      "  (67, 68)       0             0\n",
      "  (68, 68)       0             0\n",
      "  (68, 69)       0             0\n",
      "  (69, 69)       0             0\n",
      "  (69, 70)       0             0\n",
      "  (70, 70)       0             0\n",
      "  (70, 71)       0             0\n",
      "  (71, 71)       0             0\n",
      "  (71, 72)       0             0\n",
      "  (72, 72)       0             0\n",
      "  (72, 73)       0             0\n",
      "  (73, 73)       0             0\n",
      "  (73, 74)       0             0\n",
      "  (74, 74)       0             0\n",
      "  (74, 75)       0             0\n",
      "  (75, 75)       0             0\n",
      "  (75, 76)       0             0\n",
      "  (76, 76)       0             0\n",
      "  (76, 77)       0             0\n",
      "  (77, 77)       0             0\n",
      "  (77, 78)       0             0\n",
      "  (78, 78)       0             0\n",
      "  (78, 79)       0             0\n",
      "  (79, 79)       0             0\n",
      "  (79, 80)       0             0\n",
      "  (80, 80)       0             0\n",
      "  (80, 81)       0             0\n",
      "  (81, 81)       0             0\n",
      "  (81, 82)       0             0\n",
      "  (82, 82)       0             0\n",
      "  (82, 83)       0             0\n",
      "  (83, 83)       0             0\n",
      "  (83, 84)       0             0\n",
      "  (84, 84)       0             0\n",
      "  (84, 85)       0             0\n",
      "  (85, 85)       0             0\n",
      "  (85, 86)       0             0\n",
      "  (86, 86)       0             0\n",
      "  (86, 87)       0             0\n",
      "  (87, 87)       0             0\n",
      "  (87, 88)       0             0\n",
      "  (88, 88)       0             0\n",
      "  (88, 89)       0             0\n",
      "  (89, 89)       0             0\n",
      "  (89, 90)       0             0\n",
      "  (90, 90)       0             0\n",
      "  (90, 91)       0             0\n",
      "  (91, 91)       0             0\n",
      "  (91, 92)       0             0\n",
      "  (92, 92)       0             0\n",
      "  (92, 93)       0             0\n",
      "  (93, 93)       0             0\n",
      "  (93, 94)       0             0\n",
      "  (94, 94)       0             0\n",
      "  (94, 95)       0             0\n",
      "  (95, 95)       0             0\n",
      "  (95, 96)       0             0\n",
      "  (96, 96)       0             0\n",
      "  (96, 97)       0             0\n",
      "  (97, 97)       0             0\n",
      "  (97, 98)       0             0\n",
      "  (98, 98)       0             0\n",
      "  (98, 99)       0             0\n",
      "  (99, 99)       0             0\n",
      " (99, 100)       0             0\n",
      "(100, 100)       0             0\n",
      "(100, 101)       0             0\n",
      "(101, 101)       0             0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Utilisé pour afficher les données en tableau lisible\n",
    "\n",
    "# Conversion en liste pour un affichage plus lisible\n",
    "binary_span_scores_list = masked_binary_span_scores.cpu().numpy().tolist()\n",
    "\n",
    "# Affichage structuré avec les spans associés\n",
    "print(\"Binary Span Scores with Spans:\")\n",
    "for i, example in enumerate(binary_span_scores_list):  # Pour chaque exemple dans le batch\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    \n",
    "    # Création d'une structure tabulaire\n",
    "    table_data = []\n",
    "    for j, span_scores in enumerate(example):  # Pour chaque span dans l'exemple\n",
    "        associated_span = spans[j]  # Associer le span avec l'indice\n",
    "        # Ajout des données du span et des scores associés\n",
    "        table_data.append([associated_span] + span_scores[:len(entity_types_to_detect)])\n",
    "    \n",
    "    # Création d'un DataFrame pour une meilleure lisibilité\n",
    "    columns = [\"Span\"] + entity_types_to_detect  # Colonnes : Span + types d'entités\n",
    "    df = pd.DataFrame(table_data, columns=columns)\n",
    "    print(df.to_string(index=False))  # Affichage sans l'index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "  Span  person  organization\n",
      "(0, 0)       0             1\n",
      "(0, 1)       0             1\n",
      "(1, 1)       0             1\n",
      "(1, 2)       0             0\n",
      "(2, 2)       0             0\n",
      "(2, 3)       0             0\n",
      "(3, 3)       0             0\n",
      "(3, 4)       0             0\n",
      "(4, 4)       0             1\n",
      "(4, 5)       0             1\n",
      "(5, 5)       0             1\n"
     ]
    }
   ],
   "source": [
    "# Longueur limite\n",
    "max_index = len(first_subtoken_ids)\n",
    "\n",
    "# Filtrage et affichage des résultats\n",
    "for i, example in enumerate(binary_span_scores_list):\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    \n",
    "    # Création d'une structure tabulaire\n",
    "    table_data = []\n",
    "    for j, span_scores in enumerate(example):\n",
    "        associated_span = spans[j]\n",
    "        if associated_span[0] < max_index and associated_span[1] < max_index:  # Condition de filtre\n",
    "            table_data.append([associated_span] + span_scores[:len(entity_types_to_detect)])\n",
    "    \n",
    "    # Création du DataFrame\n",
    "    columns = [\"Span\"] + entity_types_to_detect\n",
    "    df = pd.DataFrame(table_data, columns=columns)\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vincentorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
